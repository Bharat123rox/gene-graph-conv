{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import torch\n",
    "import itertools\n",
    "from torch.autograd import Variable\n",
    "import sklearn, sklearn.model_selection, sklearn.metrics\n",
    "import numpy as np\n",
    "\n",
    "from models.model_wrapper import MLP, GCN, SLR\n",
    "from data import datasets\n",
    "from data.graph_wrapper import GeneManiaGraph\n",
    "from data.utils import record_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torrent name: TCGA_tissue_ppi.hdf5, Size: 1748.32MB\n",
      "Checking for pieces on disk: |██████████████████████████████████████████████████| 100.0% \n",
      "Found 1668 finished pieces out of 1668 total pieces.\n",
      "Converting one-hot labels to integers\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.TCGADataset()\n",
    "dataset.df = dataset.df - dataset.df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Checkpointed Results\n"
     ]
    }
   ],
   "source": [
    "# Setup the results dictionary\n",
    "filename = \"experiments/results/fig-5.pkl\"\n",
    "try:\n",
    "    results = pickle.load(open(filename, \"rb\"), encoding='latin1')\n",
    "    print(\"Loaded Checkpointed Results\")\n",
    "except Exception as e:\n",
    "    results = pd.DataFrame(columns=['auc', 'gene', 'model', 'num_genes', 'seed', 'train_size'])\n",
    "    print(\"Created a New Results Dictionary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_graph = GeneManiaGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_num_genes=[16000]#[50, 100, 200, 300, 500, 1000, 2000, 4000, 8000, 16000]\n",
    "test_size=100#1000\n",
    "search_train_size=[50]\n",
    "cuda=False\n",
    "trials=1\n",
    "search_genes = [\"IL5\"]#[\"RPL13\", \"CEBPD\", \"IL5\", \"PABPC3\", \"PSMB10\", \"RPL4\", \"RPL5\", \"RPS10\", \"RPS3\", \"S100A8\", \"S100A9\", \"TOP1\", \"C15orf40\", \"RNF138\", \"DLGAP2\", \"EVI2B\", \"ZFP82\", \"MYBL2\", \"PSMB1\", \"CISD1\", \"HLA-B\", \"SAA2\", \"IFIT1\", \"RPS3A\", \"TP53\", \"TNF\", \"EGFR\"]\n",
    "model_list = [              \n",
    "               GCN(name=\"GCN_lay1_chan64_emb32_dropout_agg\", cuda=cuda, num_layer=1, channels=64, embedding=32, pooling=\"hierarchy\"),\n",
    "\n",
    "              #GCN(name=\"GCN_lay20_chan32_emb32_dropout_pool\", cuda=cuda, num_layer=4, channels=32, embedding=32, prepool_extralayers=5, pooling=\"hierarchy\"),\n",
    "#               MLP(name=\"MLP_lay2_chan512_dropout\", cuda=cuda, dropout=True, num_layer=2, channels=512),\n",
    "#               MLP(name=\"MLP_lay2_chan512\", cuda=cuda, dropout=False, num_layer=2, channels=512),\n",
    "#               SLR(name=\"SLR_lambda1_l11\", cuda=cuda)\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "todo: 1\n",
      "done: 970\n"
     ]
    }
   ],
   "source": [
    "# Create the set of all experiment ids and see which are left to do\n",
    "model_names = [model.name for model in model_list]\n",
    "columns = [\"gene\", \"model\", \"num_genes\", \"train_size\", \"seed\"]\n",
    "all_exp_ids = [x for x in itertools.product(search_genes, model_names, search_num_genes, search_train_size, range(trials))]\n",
    "all_exp_ids = pd.DataFrame(all_exp_ids, columns=columns)\n",
    "all_exp_ids.index = [\"-\".join(map(str, tup[1:])) for tup in all_exp_ids.itertuples(name=None)]\n",
    "results_exp_ids = results[columns].copy()\n",
    "results_exp_ids.index = [\"-\".join(map(str, tup[1:])) for tup in results_exp_ids.itertuples(name=None)]\n",
    "intersection_ids = all_exp_ids.index.intersection(results_exp_ids.index)\n",
    "todo = all_exp_ids.drop(intersection_ids).to_dict(orient=\"records\")\n",
    "print(\"todo: \" + str(len(todo)))\n",
    "print(\"done: \" + str(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'GCN_lay1_chan64_emb32_dropout_agg', 'seed': 0, 'num_genes': 16000, 'gene': 'IL5', 'train_size': 50}\n",
      "['IL5', 'IL5RA', 'CXCL8', 'CSF2RB', 'SDCBP', 'GRB2', 'JAK2', 'UNC119', 'JAK1', 'GNAI2']\n",
      "before cached_cluster: 1547068057.369547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinweiss/code/academic/conv-graph/venv/lib/python3.5/site-packages/sklearn/cluster/hierarchical.py:831: UserWarning: Persisting input arguments took 2.35s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **kwargs)\n",
      "/Users/martinweiss/code/academic/conv-graph/models/graph_layers.py:113: UserWarning: Persisting input arguments took 7.62s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  ids = cached_cluster(n_clusters, self.adj, current_adj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_adj.sum(): 15397.205562488463\n",
      "adj.sum(): 31113.9329677256\n",
      "n_clusters.sum(): 8000\n",
      "after cached_cluster: 1547068519.489957\n",
      "> /Users/martinweiss/code/academic/conv-graph/models/model_wrapper.py(66)fit()\n",
      "-> x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(X, y, stratify=y, train_size=self.train_valid_split, test_size=1-self.train_valid_split, random_state=self.seed)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /Users/martinweiss/code/academic/conv-graph/models/model_wrapper.py(69)fit()\n",
      "-> x_train = torch.FloatTensor(np.expand_dims(x_train, axis=2))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /Users/martinweiss/code/academic/conv-graph/models/model_wrapper.py(70)fit()\n",
      "-> x_valid = torch.FloatTensor(np.expand_dims(x_valid, axis=2))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /Users/martinweiss/code/academic/conv-graph/models/model_wrapper.py(71)fit()\n",
      "-> y_train = torch.FloatTensor(y_train.values)\n"
     ]
    }
   ],
   "source": [
    "for row in todo:\n",
    "    print(row)\n",
    "    start_time = time.time()\n",
    "    gene = row[\"gene\"]\n",
    "    model_name = row[\"model\"]\n",
    "    seed = row[\"seed\"]\n",
    "    num_genes = row[\"num_genes\"]\n",
    "    train_size = row[\"train_size\"]\n",
    "\n",
    "    model = [x for x in model_list if x.name == model_name][0]\n",
    "\n",
    "    experiment = {\n",
    "        \"gene\": gene,\n",
    "        \"model\": model.name,\n",
    "        \"num_genes\": num_genes,\n",
    "        \"train_size\": train_size,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "\n",
    "    dataset.labels = dataset.df[gene].where(dataset.df[gene] > 0).notnull().astype(\"int\")\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset.df, dataset.labels, stratify=dataset.labels, train_size=train_size, test_size=test_size, random_state=seed)\n",
    "\n",
    "    neighbors = gene_graph.bfs_sample_neighbors(gene, num_genes)\n",
    "\n",
    "    X_train = X_train[list(neighbors.nodes)].copy()\n",
    "    X_test = X_test[list(neighbors.nodes)].copy()\n",
    "    X_train[gene] = 1\n",
    "    X_test[gene] = 1\n",
    "    print(list(neighbors.nodes)[:10])\n",
    "    adj = np.asarray(nx.to_numpy_matrix(neighbors))\n",
    "    model.fit(X_train, y_train, adj=adj)\n",
    "\n",
    "    x_test = Variable(torch.FloatTensor(np.expand_dims(X_test.values, axis=2)), requires_grad=False).float()\n",
    "    if cuda:\n",
    "        x_test.cuda()\n",
    "    y_hat = model.predict(x_test)[:,1].data.cpu().numpy()\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, np.asarray(y_hat).flatten())\n",
    "\n",
    "    model.best_model = None # cleanup\n",
    "    experiment[\"auc\"] = auc\n",
    "    results = record_result(results, experiment, filename)\n",
    "    print(\"time elapsed for genes: \" +str(num_genes) + \" : \" + str(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.load(\"/tmp/D.npy\")\n",
    "adj = np.load(\"/tmp/adj.npy\")\n",
    "D_inv = np.diag(1. / np.sqrt(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.7 s ± 1.77 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "norm_transform = D_inv.dot(adj).dot(D_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = sklearn.cluster.AgglomerativeClustering(n_clusters=4000, affinity='euclidean',\n",
    "                                                                      memory='/tmp', connectivity=(adj > 0.).astype(int),\n",
    "                                                                      compute_full_tree='auto', linkage='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinweiss/code/academic/conv-graph/venv/lib/python3.5/site-packages/sklearn/cluster/hierarchical.py:831: UserWarning: Persisting input arguments took 1.50s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "c = asdf.fit_predict(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinweiss/code/academic/conv-graph/venv/lib/python3.5/site-packages/sklearn/cluster/hierarchical.py:831: UserWarning: Persisting input arguments took 0.87s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "c = clustering.fit_predict((adj > 0.).astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = clustering.fit_predict((adj > 0.).astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for plot_gene in search_genes:\n",
    "\n",
    "    %matplotlib inline\n",
    "    plt.rcParams['figure.figsize'] = (7.5, 3.6)\n",
    "    plot_train_size = 50\n",
    "\n",
    "    subset = results[(results.train_size==plot_train_size) & \n",
    "                      (results.gene==plot_gene) & \n",
    "                      (results.num_genes!=400) &      \n",
    "                      (results.num_genes> 0)]\n",
    "\n",
    "\n",
    "    q = subset.groupby(['model','num_genes'])['auc']\n",
    "\n",
    "    todo = list(subset[\"model\"].unique())\n",
    "    linestyles = ['-', '-', '--', '-.', ':']\n",
    "    for ls, model in enumerate(sorted(todo)):\n",
    "        index = list(q.mean()[model].index)\n",
    "        mean = q.mean()[model]\n",
    "        stderr = q.std()[model]/np.sqrt(q.count()[model])\n",
    "        displayname = model.replace(\"CGN\",\"GCN\")\n",
    "        displayname = displayname.replace(\"SLR\", \"SNLR\")\n",
    "        plt.errorbar(index, mean,label=displayname, xerr=0, yerr=stderr, ls=linestyles[ls])\n",
    "\n",
    "    plt.title(\"Gene Inference \" + plot_gene + \" (train_size:\" + str(plot_train_size) +\")\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.xlabel(\"Number of genes\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xticks(sorted(subset[\"num_genes\"].unique()))\n",
    "    formatter = matplotlib.ticker.ScalarFormatter()\n",
    "    plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "    plt.legend();\n",
    "    fd = len(list(gene_graph.nx_graph.neighbors(plot_gene)))\n",
    "    print fd\n",
    "    if fd > 50:\n",
    "        plt.axvline(fd, ymin=0.4, ymax=1.0, c=\"black\")\n",
    "        c = plt.ylim()\n",
    "        plt.text(fd*1.05,c[1]-((c[1]-c[0])*0.2),'First Degree',rotation=90)\n",
    "\n",
    "\n",
    "    plt.savefig(\"experiments/results/sgi-\" + plot_gene + \"-\" + \"train\" + str(plot_train_size) + \".png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
