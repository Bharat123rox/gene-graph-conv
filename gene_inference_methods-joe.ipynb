{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from itertools import repeat\n",
    "import data, data.gene_datasets\n",
    "import sklearn, sklearn.model_selection, sklearn.metrics, sklearn.linear_model, sklearn.neural_network, sklearn.tree\n",
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import gene_inference\n",
    "#from gene_inference.infer_genes import infer_all_genes, sample_neighbors\n",
    "import models, models.graphLayer\n",
    "from models.models import CGN\n",
    "import data, data.gene_datasets\n",
    "from data.graph import Graph\n",
    "from data.utils import split_dataset\n",
    "import optimization\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from analysis.metrics import record_metrics_for_epoch\n",
    "import analysis\n",
    "reload(analysis.metrics)\n",
    "reload(gene_inference);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting one-hot labels to integers\n"
     ]
    }
   ],
   "source": [
    "#tcgatissue = data.gene_datasets.TCGATissue(data_dir='./genomics/TCGA/', data_file='TCGA_tissue_ppi.hdf5')\n",
    "tcgatissue = data.gene_datasets.TCGATissue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "opt = Object()\n",
    "opt.seed = 0\n",
    "opt.nb_class = None\n",
    "opt.nb_examples = None\n",
    "opt.nb_nodes = None\n",
    "opt.graph = \"pathway\"\n",
    "opt.dataset = tcgatissue\n",
    "opt.add_self = True\n",
    "opt.norm_adj = True\n",
    "opt.add_connectivity = False\n",
    "opt.num_layer = 1\n",
    "opt.cuda = True\n",
    "opt.pool_graph = \"ignore\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "path = \"/data/lisa/data/genomics/graph/pancan-tissue-graph.hdf5\"\n",
    "graph.load_graph(path)\n",
    "#graph.intersection_with(tcgatissue)\n",
    "g = nx.from_numpy_matrix(graph.adj)\n",
    "mapping = dict(zip(range(0, len(tcgatissue.df.columns)), tcgatissue.df.columns))\n",
    "g = nx.relabel_nodes(g, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_neighbors(g, gene, num_neighbors, include_self=True):\n",
    "    results = set([])\n",
    "    if include_self:\n",
    "        results = set([gene])\n",
    "    all_nodes = set(g.nodes)\n",
    "    first_degree = set(g.neighbors(gene))\n",
    "    second_degree = set()\n",
    "    for x in g.neighbors(gene):\n",
    "        second_degree = second_degree.union(set(g.neighbors(x)))\n",
    "    while len(results) < num_neighbors:\n",
    "        if len(first_degree) - len(results) > 0:\n",
    "            unique = first_degree - results\n",
    "            results.add(np.random.choice(list(unique)))\n",
    "        elif len(second_degree) - len(results) > 0:\n",
    "            unique = second_degree - results\n",
    "            results.add(np.random.choice(list(unique)))\n",
    "        else:\n",
    "            unique = all_nodes - results\n",
    "            results.add(np.random.choice(list(unique)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample_neighbors(g, \"RPL5\", 5, include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn, sklearn.model_selection, sklearn.metrics, sklearn.linear_model, sklearn.neural_network, sklearn.tree\n",
    "import numpy as np\n",
    "\n",
    "class Method:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class SkLearn(Method):\n",
    "    \n",
    "    def __init__(self, model, penalty=False):\n",
    "        self.model = model\n",
    "        self.penalty = penalty\n",
    "        \n",
    "    def loop(self, dataset, seed, train_size, test_size, adj=None):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset.df, dataset.labels, stratify=dataset.labels, train_size=train_size, test_size=test_size, random_state=seed)\n",
    "\n",
    "        if self.model == \"LR\":\n",
    "            model = sklearn.linear_model.LogisticRegression()\n",
    "            if self.penalty:\n",
    "                model = sklearn.linear_model.LogisticRegression(penalty='l1', tol=0.0001)\n",
    "        elif self.model == \"DT\":\n",
    "            model = sklearn.tree.DecisionTreeClassifier()\n",
    "        elif self.model == \"MLP\":\n",
    "            model = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(32,3), learning_rate_init=0.001, early_stopping=False,  max_iter=1000)\n",
    "        else:\n",
    "            print \"incorrect label\"\n",
    "        \n",
    "        model = model.fit(X_train, y_train)\n",
    "        return sklearn.metrics.roc_auc_score(y_test, model.predict(X_test))\n",
    "\n",
    "\n",
    "class PyTorch(Method):    \n",
    "    \n",
    "    def __init__(self, model, num_epochs=100, num_channel=64, num_layer=3, add_emb=32, use_gate=False, dropout=True, cuda=True):\n",
    "        self.model = model\n",
    "        self.batch_size = 10\n",
    "        self.num_channel = num_channel\n",
    "        self.num_layer = num_layer\n",
    "        self.add_emb = add_emb\n",
    "        self.use_gate = use_gate\n",
    "        self.dropout = dropout\n",
    "        self.cuda = cuda\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def loop(self, dataset, seed, train_size, test_size, adj=None):\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset.df, dataset.labels, stratify=dataset.labels, train_size=train_size, test_size=test_size, random_state=seed)\n",
    "    \n",
    "        #split train into valid and train\n",
    "        local_X_train, local_X_valid, local_y_train, local_y_valid = sklearn.model_selection.train_test_split(X_train, y_train, stratify=y_train, train_size=0.60, random_state=seed)\n",
    "    \n",
    "    \n",
    "        local_X_train = torch.FloatTensor(np.expand_dims(local_X_train, axis=2))\n",
    "        local_X_valid = torch.FloatTensor(np.expand_dims(local_X_valid, axis=2))\n",
    "        X_test = torch.FloatTensor(np.expand_dims(X_test, axis=2))\n",
    "        \n",
    "        local_y_train = torch.FloatTensor(local_y_train)\n",
    "\n",
    "        criterion = optimization.get_criterion(dataset)\n",
    "        \n",
    "        patience = 20\n",
    "        opt.num_layer = self.num_layer\n",
    "        adj_transform, aggregate_function = models.graphLayer.get_transform(opt, adj)\n",
    "        model = models.models.CGN(\n",
    "                nb_nodes=len(dataset.df.columns), \n",
    "                input_dim=1,\n",
    "                channels=[self.num_channel] * self.num_layer,\n",
    "                adj=adj,\n",
    "                out_dim=2,\n",
    "                on_cuda=self.cuda,\n",
    "                add_emb=self.add_emb,\n",
    "                transform_adj=adj_transform,\n",
    "                aggregate_adj=aggregate_function,\n",
    "                use_gate=self.use_gate,\n",
    "                dropout=self.dropout,\n",
    "                )\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            model.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        max_valid = 0\n",
    "        for t in range(0, self.num_epochs):\n",
    "            start_timer = time.time()\n",
    "            \n",
    "            if self.cuda:\n",
    "                model.cuda()\n",
    "                model.on_cuda = True\n",
    "            \n",
    "            for base_x in range(0,local_X_train.shape[0], self.batch_size):\n",
    "                inputs, labels = local_X_train[base_x:base_x+self.batch_size], local_y_train[base_x:base_x+self.batch_size]\n",
    "\n",
    "                inputs = Variable(inputs, requires_grad=False).float()\n",
    "                if self.cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                model.train()\n",
    "                y_pred = model(inputs)\n",
    "\n",
    "                # Compute and print loss\n",
    "                crit_loss = optimization.compute_loss(criterion, y_pred, labels)\n",
    "                total_loss = crit_loss\n",
    "\n",
    "                # Zero gradients, perform a backward pass, and update the weights.\n",
    "                optimizer.zero_grad()\n",
    "                crit_loss.backward()\n",
    "                optimizer.step()\n",
    "                model.eval()\n",
    "            time_this_epoch = time.time() - start_timer\n",
    "            \n",
    "            \n",
    "            auc = {}\n",
    "            if self.cuda:\n",
    "                model.cpu()\n",
    "                model.on_cuda = False\n",
    "\n",
    "            auc['train'] = sklearn.metrics.roc_auc_score(local_y_train.numpy(), model(Variable(local_X_train.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            auc['valid'] = sklearn.metrics.roc_auc_score(local_y_valid, model(Variable(local_X_valid.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            auc['test'] = sklearn.metrics.roc_auc_score(y_test, model(Variable(X_test.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            \n",
    "            summary = [ t, crit_loss.data[0], auc['train'], auc['valid'], time_this_epoch ]\n",
    "            summary = \"epoch {}, cross_loss: {:.03f}, auc_train: {:0.3f}, auc_valid:{:0.3f}, time: {:.02f} sec\".format(*summary)\n",
    "            print summary\n",
    "\n",
    "            patience = patience - 1\n",
    "            if patience == 0:\n",
    "                break\n",
    "            if max_valid < auc['valid']:\n",
    "                max_valid = auc['valid']\n",
    "            if max_valid > auc['valid'] and t > 15:\n",
    "                #scores.append(auc['test']) \n",
    "                return auc['test']\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def method_comparison(results, dataset, models, gene, max_genes, trials, train_size, test_size):\n",
    "    \n",
    "    dataset = data.gene_datasets.TCGATissue()\n",
    "    dataset.df = dataset.df - dataset.df.mean()\n",
    "    \n",
    "    mean = dataset.df[gene].mean()\n",
    "    dataset.labels = [1 if x > mean else 0 for x in dataset.df[gene]]\n",
    "    full_df = dataset.df.copy(deep=True)\n",
    "    \n",
    "    print \"Max ex \", int(np.log2(max_genes))+1\n",
    "    for ex in range(4, int(np.log2(max_genes))+1):\n",
    "        \n",
    "        num_genes = 2**ex\n",
    "        num_genes = np.min([num_genes, tcgatissue.df.shape[1]])\n",
    "        print ex, num_genes\n",
    "        \n",
    "        neighbors = sample_neighbors(g, gene, num_genes, include_self=False)\n",
    "        print \"neighbors\", len(neighbors)\n",
    "        \n",
    "        if gene in neighbors:\n",
    "            neighbors.remove(gene)\n",
    "\n",
    "        dataset.df = dataset.df[list(neighbors)]\n",
    "        dataset.data = dataset.df.as_matrix()\n",
    "        \n",
    "        neighborhood = np.asarray(nx.to_numpy_matrix(nx.Graph(g.subgraph(neighbors))))\n",
    "        \n",
    "        for model in models:\n",
    "            for seed in range(trials):\n",
    "            \n",
    "                #have we already done it?\n",
    "                already_done = results[\"df\"][(results[\"df\"].gene_name == gene) & \n",
    "                                             (results[\"df\"].model == model['key']) &\n",
    "                                             (results[\"df\"].num_genes == num_genes) &\n",
    "                                             (results[\"df\"].seed == seed) &\n",
    "                                             (results[\"df\"].train_size == train_size)].shape[0] > 0\n",
    "\n",
    "                if already_done:\n",
    "                    print \"already done:\", model['key'], num_genes, seed\n",
    "                    continue\n",
    "                print \"doing:\", model['key'], num_genes, seed\n",
    "\n",
    "                result = model['method'].loop(dataset=dataset, seed=seed, train_size=train_size, test_size=test_size, adj=neighborhood)\n",
    "\n",
    "                experiment = {\"gene_name\": gene,\n",
    "                        \"model\": model['key'],\n",
    "                        \"num_genes\": num_genes, \n",
    "                        \"seed\":seed,\n",
    "                        \"train_size\": train_size,\n",
    "                        \"auc\":result\n",
    "                        }\n",
    "\n",
    "                results[\"df\"] = results[\"df\"].append(experiment, ignore_index=True)\n",
    "                pickle.dump(results, open(\"results-temp.pkl\", \"wb\"))\n",
    "        dataset.df = full_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data\n",
    "reload(data)\n",
    "#reload(models.models)\n",
    "reload(gene_inference)\n",
    "#reload(gene_inference.models)\n",
    "reload(analysis.metrics)\n",
    "import pickle\n",
    "\n",
    "m = [\n",
    "    {'key': 'LR-L1', 'method': SkLearn(\"LR\", penalty=True)},\n",
    "#    {'key': 'MLP', 'method': mlp},\n",
    "    {'key': 'DT', 'method': SkLearn(\"DT\")},\n",
    "   {'key': 'CGN_3_layer_64_channel_emb_32_dropout', 'method': PyTorch(\"CGN\")},\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results = {\"df\": pd.DataFrame(columns=['auc','gene_name', 'model', 'num_genes', 'seed', 'train_size'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#results = pickle.load(open(\"results-temp.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting one-hot labels to integers\n",
      "Max ex  10\n",
      "4 16\n",
      "neighbors 16\n",
      "already done: LR-L1 16 0\n",
      "already done: DT 16 0\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 16 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/cohenjos/.local/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.683, auc_train: 0.731, auc_valid:0.690, time: 4.13 sec\n",
      "epoch 1, cross_loss: 0.697, auc_train: 0.853, auc_valid:0.770, time: 0.04 sec\n",
      "epoch 2, cross_loss: 0.729, auc_train: 0.895, auc_valid:0.795, time: 0.03 sec\n",
      "epoch 3, cross_loss: 0.716, auc_train: 0.907, auc_valid:0.798, time: 0.03 sec\n",
      "epoch 4, cross_loss: 0.690, auc_train: 0.923, auc_valid:0.797, time: 0.03 sec\n",
      "epoch 5, cross_loss: 0.684, auc_train: 0.932, auc_valid:0.807, time: 0.03 sec\n",
      "epoch 6, cross_loss: 0.679, auc_train: 0.937, auc_valid:0.818, time: 0.03 sec\n",
      "epoch 7, cross_loss: 0.680, auc_train: 0.938, auc_valid:0.810, time: 0.03 sec\n",
      "epoch 8, cross_loss: 0.601, auc_train: 0.937, auc_valid:0.800, time: 0.03 sec\n",
      "epoch 9, cross_loss: 0.578, auc_train: 0.935, auc_valid:0.788, time: 0.03 sec\n",
      "epoch 10, cross_loss: 0.470, auc_train: 0.931, auc_valid:0.782, time: 0.03 sec\n",
      "epoch 11, cross_loss: 0.382, auc_train: 0.934, auc_valid:0.780, time: 0.03 sec\n",
      "epoch 12, cross_loss: 0.335, auc_train: 0.941, auc_valid:0.783, time: 0.03 sec\n",
      "epoch 13, cross_loss: 0.163, auc_train: 0.949, auc_valid:0.790, time: 0.03 sec\n",
      "epoch 14, cross_loss: 0.153, auc_train: 0.953, auc_valid:0.792, time: 0.03 sec\n",
      "epoch 15, cross_loss: 0.178, auc_train: 0.952, auc_valid:0.805, time: 0.03 sec\n",
      "epoch 16, cross_loss: 0.098, auc_train: 0.959, auc_valid:0.812, time: 0.03 sec\n",
      "5 32\n",
      "neighbors 32\n",
      "already done: LR-L1 32 0\n",
      "already done: DT 32 0\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 32 0\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.693, auc_train: 0.800, auc_valid:0.710, time: 0.11 sec\n",
      "epoch 1, cross_loss: 0.711, auc_train: 0.869, auc_valid:0.670, time: 0.03 sec\n",
      "epoch 2, cross_loss: 0.739, auc_train: 0.892, auc_valid:0.672, time: 0.03 sec\n",
      "epoch 3, cross_loss: 0.715, auc_train: 0.905, auc_valid:0.675, time: 0.03 sec\n",
      "epoch 4, cross_loss: 0.692, auc_train: 0.912, auc_valid:0.695, time: 0.03 sec\n",
      "epoch 5, cross_loss: 0.683, auc_train: 0.915, auc_valid:0.720, time: 0.04 sec\n",
      "epoch 6, cross_loss: 0.640, auc_train: 0.915, auc_valid:0.732, time: 0.03 sec\n",
      "epoch 7, cross_loss: 0.624, auc_train: 0.915, auc_valid:0.732, time: 0.03 sec\n",
      "epoch 8, cross_loss: 0.545, auc_train: 0.914, auc_valid:0.743, time: 0.03 sec\n",
      "epoch 9, cross_loss: 0.527, auc_train: 0.914, auc_valid:0.740, time: 0.03 sec\n",
      "epoch 10, cross_loss: 0.388, auc_train: 0.913, auc_valid:0.745, time: 0.03 sec\n",
      "epoch 11, cross_loss: 0.337, auc_train: 0.917, auc_valid:0.745, time: 0.04 sec\n",
      "epoch 12, cross_loss: 0.305, auc_train: 0.919, auc_valid:0.768, time: 0.03 sec\n",
      "epoch 13, cross_loss: 0.380, auc_train: 0.920, auc_valid:0.770, time: 0.03 sec\n",
      "epoch 14, cross_loss: 0.244, auc_train: 0.928, auc_valid:0.775, time: 0.03 sec\n",
      "epoch 15, cross_loss: 0.298, auc_train: 0.933, auc_valid:0.782, time: 0.03 sec\n",
      "epoch 16, cross_loss: 0.361, auc_train: 0.935, auc_valid:0.788, time: 0.03 sec\n",
      "epoch 17, cross_loss: 0.455, auc_train: 0.940, auc_valid:0.792, time: 0.03 sec\n",
      "epoch 18, cross_loss: 0.326, auc_train: 0.941, auc_valid:0.790, time: 0.03 sec\n",
      "6 64\n",
      "neighbors 64\n",
      "already done: LR-L1 64 0\n",
      "already done: DT 64 0\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 64 0\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.714, auc_train: 0.821, auc_valid:0.847, time: 0.04 sec\n",
      "epoch 1, cross_loss: 0.730, auc_train: 0.901, auc_valid:0.847, time: 2.99 sec\n",
      "epoch 2, cross_loss: 0.698, auc_train: 0.928, auc_valid:0.838, time: 0.04 sec\n",
      "epoch 3, cross_loss: 0.679, auc_train: 0.929, auc_valid:0.818, time: 0.04 sec\n",
      "epoch 4, cross_loss: 0.677, auc_train: 0.931, auc_valid:0.812, time: 0.04 sec\n",
      "epoch 5, cross_loss: 0.674, auc_train: 0.931, auc_valid:0.815, time: 0.04 sec\n",
      "epoch 6, cross_loss: 0.655, auc_train: 0.933, auc_valid:0.810, time: 0.04 sec\n",
      "epoch 7, cross_loss: 0.555, auc_train: 0.933, auc_valid:0.807, time: 0.04 sec\n",
      "epoch 8, cross_loss: 0.449, auc_train: 0.933, auc_valid:0.797, time: 0.04 sec\n",
      "epoch 9, cross_loss: 0.356, auc_train: 0.933, auc_valid:0.792, time: 0.04 sec\n",
      "epoch 10, cross_loss: 0.357, auc_train: 0.935, auc_valid:0.787, time: 0.04 sec\n",
      "epoch 11, cross_loss: 0.215, auc_train: 0.939, auc_valid:0.785, time: 0.04 sec\n",
      "epoch 12, cross_loss: 0.205, auc_train: 0.940, auc_valid:0.790, time: 0.04 sec\n",
      "epoch 13, cross_loss: 0.210, auc_train: 0.945, auc_valid:0.797, time: 0.04 sec\n",
      "epoch 14, cross_loss: 0.199, auc_train: 0.948, auc_valid:0.800, time: 0.04 sec\n",
      "epoch 15, cross_loss: 0.232, auc_train: 0.950, auc_valid:0.805, time: 0.04 sec\n",
      "epoch 16, cross_loss: 0.185, auc_train: 0.957, auc_valid:0.805, time: 0.04 sec\n",
      "7 128\n",
      "neighbors 128\n",
      "already done: LR-L1 128 0\n",
      "already done: DT 128 0\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 128 0\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.755, auc_train: 0.918, auc_valid:0.715, time: 0.10 sec\n",
      "epoch 1, cross_loss: 0.750, auc_train: 0.925, auc_valid:0.742, time: 2.92 sec\n",
      "epoch 2, cross_loss: 0.728, auc_train: 0.935, auc_valid:0.748, time: 3.06 sec\n",
      "epoch 3, cross_loss: 0.673, auc_train: 0.934, auc_valid:0.772, time: 1.15 sec\n",
      "epoch 4, cross_loss: 0.648, auc_train: 0.938, auc_valid:0.782, time: 0.04 sec\n",
      "epoch 5, cross_loss: 0.658, auc_train: 0.939, auc_valid:0.790, time: 0.04 sec\n",
      "epoch 6, cross_loss: 0.592, auc_train: 0.939, auc_valid:0.792, time: 0.04 sec\n",
      "epoch 7, cross_loss: 0.540, auc_train: 0.940, auc_valid:0.797, time: 0.05 sec\n",
      "epoch 8, cross_loss: 0.456, auc_train: 0.944, auc_valid:0.802, time: 0.04 sec\n",
      "epoch 9, cross_loss: 0.344, auc_train: 0.945, auc_valid:0.810, time: 0.04 sec\n",
      "epoch 10, cross_loss: 0.219, auc_train: 0.951, auc_valid:0.815, time: 0.04 sec\n",
      "epoch 11, cross_loss: 0.201, auc_train: 0.953, auc_valid:0.830, time: 0.04 sec\n",
      "epoch 12, cross_loss: 0.263, auc_train: 0.960, auc_valid:0.848, time: 0.04 sec\n",
      "epoch 13, cross_loss: 0.229, auc_train: 0.960, auc_valid:0.857, time: 0.04 sec\n",
      "epoch 14, cross_loss: 0.278, auc_train: 0.969, auc_valid:0.865, time: 0.04 sec\n",
      "epoch 15, cross_loss: 0.221, auc_train: 0.976, auc_valid:0.863, time: 0.08 sec\n",
      "epoch 16, cross_loss: 0.188, auc_train: 0.979, auc_valid:0.867, time: 0.05 sec\n",
      "epoch 17, cross_loss: 0.150, auc_train: 0.984, auc_valid:0.867, time: 0.04 sec\n",
      "epoch 18, cross_loss: 0.142, auc_train: 0.993, auc_valid:0.865, time: 0.04 sec\n",
      "8 256\n",
      "neighbors 256\n",
      "already done: LR-L1 256 0\n",
      "already done: DT 256 0\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 256 0\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.936, auc_train: 0.930, auc_valid:0.800, time: 0.26 sec\n",
      "epoch 1, cross_loss: 0.733, auc_train: 0.945, auc_valid:0.830, time: 0.94 sec\n",
      "epoch 2, cross_loss: 0.656, auc_train: 0.948, auc_valid:0.843, time: 0.08 sec\n",
      "epoch 3, cross_loss: 0.668, auc_train: 0.948, auc_valid:0.830, time: 0.09 sec\n",
      "epoch 4, cross_loss: 0.702, auc_train: 0.948, auc_valid:0.827, time: 0.09 sec\n",
      "epoch 5, cross_loss: 0.615, auc_train: 0.947, auc_valid:0.827, time: 0.08 sec\n",
      "epoch 6, cross_loss: 0.533, auc_train: 0.947, auc_valid:0.823, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.448, auc_train: 0.948, auc_valid:0.820, time: 0.08 sec\n",
      "epoch 8, cross_loss: 0.324, auc_train: 0.949, auc_valid:0.823, time: 0.08 sec\n",
      "epoch 9, cross_loss: 0.228, auc_train: 0.949, auc_valid:0.825, time: 0.08 sec\n",
      "epoch 10, cross_loss: 0.172, auc_train: 0.956, auc_valid:0.825, time: 0.08 sec\n",
      "epoch 11, cross_loss: 0.190, auc_train: 0.962, auc_valid:0.827, time: 0.08 sec\n",
      "epoch 12, cross_loss: 0.162, auc_train: 0.968, auc_valid:0.847, time: 0.08 sec\n",
      "epoch 13, cross_loss: 0.240, auc_train: 0.976, auc_valid:0.847, time: 0.08 sec\n",
      "epoch 14, cross_loss: 0.082, auc_train: 0.981, auc_valid:0.855, time: 0.29 sec\n",
      "epoch 15, cross_loss: 0.141, auc_train: 0.989, auc_valid:0.860, time: 0.09 sec\n",
      "epoch 16, cross_loss: 0.124, auc_train: 0.994, auc_valid:0.857, time: 0.08 sec\n",
      "9 512\n",
      "neighbors 512\n",
      "already done: LR-L1 512 0\n",
      "already done: DT 512 0\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 512 0\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 1.447, auc_train: 0.902, auc_valid:0.672, time: 0.25 sec\n",
      "epoch 1, cross_loss: 0.601, auc_train: 0.937, auc_valid:0.725, time: 2.28 sec\n",
      "epoch 2, cross_loss: 0.771, auc_train: 0.940, auc_valid:0.745, time: 0.15 sec\n",
      "epoch 3, cross_loss: 0.789, auc_train: 0.947, auc_valid:0.750, time: 0.15 sec\n",
      "epoch 4, cross_loss: 0.655, auc_train: 0.951, auc_valid:0.760, time: 0.31 sec\n",
      "epoch 5, cross_loss: 0.616, auc_train: 0.951, auc_valid:0.768, time: 0.15 sec\n",
      "epoch 6, cross_loss: 0.626, auc_train: 0.953, auc_valid:0.780, time: 0.15 sec\n",
      "epoch 7, cross_loss: 0.531, auc_train: 0.952, auc_valid:0.792, time: 0.15 sec\n",
      "epoch 8, cross_loss: 0.385, auc_train: 0.951, auc_valid:0.792, time: 0.15 sec\n",
      "epoch 9, cross_loss: 0.311, auc_train: 0.956, auc_valid:0.790, time: 0.15 sec\n",
      "epoch 10, cross_loss: 0.248, auc_train: 0.963, auc_valid:0.803, time: 0.15 sec\n",
      "epoch 11, cross_loss: 0.176, auc_train: 0.978, auc_valid:0.823, time: 0.15 sec\n",
      "epoch 12, cross_loss: 0.214, auc_train: 0.988, auc_valid:0.848, time: 0.15 sec\n",
      "epoch 13, cross_loss: 0.152, auc_train: 0.992, auc_valid:0.870, time: 0.15 sec\n",
      "epoch 14, cross_loss: 0.064, auc_train: 0.999, auc_valid:0.880, time: 0.15 sec\n",
      "epoch 15, cross_loss: 0.240, auc_train: 1.000, auc_valid:0.897, time: 0.32 sec\n",
      "epoch 16, cross_loss: 0.155, auc_train: 1.000, auc_valid:0.905, time: 0.15 sec\n",
      "epoch 17, cross_loss: 0.048, auc_train: 1.000, auc_valid:0.913, time: 0.15 sec\n",
      "epoch 18, cross_loss: 0.030, auc_train: 1.000, auc_valid:0.920, time: 0.15 sec\n",
      "epoch 19, cross_loss: 0.009, auc_train: 1.000, auc_valid:0.922, time: 0.15 sec\n"
     ]
    }
   ],
   "source": [
    "method_comparison(results, tcgatissue, m, gene=\"RPL5\", max_genes=600, trials=1, train_size=100, test_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(results, open(\"results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pickle.load(open(\"results-temp.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>gene_name</th>\n",
       "      <th>model</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>seed</th>\n",
       "      <th>train_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800461</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.822363</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.832213</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.731107</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.760795</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.801509</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.773086</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.812424</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.816969</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.735812</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.768829</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.791539</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.826091</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.850818</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.835894</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.740982</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.806494</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.760899</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.811384</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.814681</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.847833</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.753697</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.784865</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.787122</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.820146</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.854851</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.837862</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.785793</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.811384</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.727018</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.767909</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.840911</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.810616</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.811256</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.808207</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.780616</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.821538</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.800549</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.749392</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.761051</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.844088</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.820554</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.779752</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.793531</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.823923</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.850594</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.765716</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.819890</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.813856</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.827004</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.742470</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.808943</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.829004</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.823987</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.777719</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.810263</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.830052</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.811792</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.784833</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.799821</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         auc gene_name  model num_genes seed train_size\n",
       "0   0.800461      RPL5  LR-L1        16    0        100\n",
       "1   0.822363      RPL5  LR-L1        16    1        100\n",
       "2   0.832213      RPL5  LR-L1        16    2        100\n",
       "3   0.731107      RPL5     DT        16    0        100\n",
       "4   0.760795      RPL5     DT        16    1        100\n",
       "5   0.801509      RPL5     DT        16    2        100\n",
       "6   0.773086      RPL5  LR-L1        32    0        100\n",
       "7   0.812424      RPL5  LR-L1        32    1        100\n",
       "8   0.816969      RPL5  LR-L1        32    2        100\n",
       "9   0.735812      RPL5     DT        32    0        100\n",
       "10  0.768829      RPL5     DT        32    1        100\n",
       "11  0.791539      RPL5     DT        32    2        100\n",
       "12  0.826091      RPL5  LR-L1        64    0        100\n",
       "13  0.850818      RPL5  LR-L1        64    1        100\n",
       "14  0.835894      RPL5  LR-L1        64    2        100\n",
       "15  0.740982      RPL5     DT        64    0        100\n",
       "16  0.806494      RPL5     DT        64    1        100\n",
       "17  0.760899      RPL5     DT        64    2        100\n",
       "18  0.811384      RPL5  LR-L1       128    0        100\n",
       "19  0.814681      RPL5  LR-L1       128    1        100\n",
       "20  0.847833      RPL5  LR-L1       128    2        100\n",
       "21  0.753697      RPL5     DT       128    0        100\n",
       "22  0.784865      RPL5     DT       128    1        100\n",
       "23  0.787122      RPL5     DT       128    2        100\n",
       "24  0.820146      RPL5  LR-L1       256    0        100\n",
       "25  0.854851      RPL5  LR-L1       256    1        100\n",
       "26  0.837862      RPL5  LR-L1       256    2        100\n",
       "27  0.785793      RPL5     DT       256    0        100\n",
       "28  0.811384      RPL5     DT       256    1        100\n",
       "29  0.727018      RPL5     DT       256    2        100\n",
       "30  0.767909      RPL5  LR-L1       512    0        100\n",
       "31  0.840911      RPL5  LR-L1       512    1        100\n",
       "32  0.810616      RPL5  LR-L1       512    2        100\n",
       "33  0.811256      RPL5     DT       512    0        100\n",
       "34  0.808207      RPL5     DT       512    1        100\n",
       "35  0.780616      RPL5     DT       512    2        100\n",
       "36  0.821538      RPL5  LR-L1        16    3        100\n",
       "37  0.800549      RPL5  LR-L1        16    4        100\n",
       "38  0.749392      RPL5     DT        16    3        100\n",
       "39  0.761051      RPL5     DT        16    4        100\n",
       "40  0.844088      RPL5  LR-L1        32    3        100\n",
       "41  0.820554      RPL5  LR-L1        32    4        100\n",
       "42  0.779752      RPL5     DT        32    3        100\n",
       "43  0.793531      RPL5     DT        32    4        100\n",
       "44  0.823923      RPL5  LR-L1        64    3        100\n",
       "45  0.850594      RPL5  LR-L1        64    4        100\n",
       "46  0.765716      RPL5     DT        64    3        100\n",
       "47  0.819890      RPL5     DT        64    4        100\n",
       "48  0.813856      RPL5  LR-L1       128    3        100\n",
       "49  0.827004      RPL5  LR-L1       128    4        100\n",
       "50  0.742470      RPL5     DT       128    3        100\n",
       "51  0.808943      RPL5     DT       128    4        100\n",
       "52  0.829004      RPL5  LR-L1       256    3        100\n",
       "53  0.823987      RPL5  LR-L1       256    4        100\n",
       "54  0.777719      RPL5     DT       256    3        100\n",
       "55  0.810263      RPL5     DT       256    4        100\n",
       "56  0.830052      RPL5  LR-L1       512    3        100\n",
       "57  0.811792      RPL5  LR-L1       512    4        100\n",
       "58  0.784833      RPL5     DT       512    3        100\n",
       "59  0.799821      RPL5     DT       512    4        100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene_name</th>\n",
       "      <th>model</th>\n",
       "      <th>num_genes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">RPL5</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">DT</th>\n",
       "      <th>16</th>\n",
       "      <td>0.760771</td>\n",
       "      <td>0.025832</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.773893</td>\n",
       "      <td>0.023490</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.778796</td>\n",
       "      <td>0.033080</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.775419</td>\n",
       "      <td>0.026961</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.782436</td>\n",
       "      <td>0.034340</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.796946</td>\n",
       "      <td>0.013723</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">LR-L1</th>\n",
       "      <th>16</th>\n",
       "      <td>0.815425</td>\n",
       "      <td>0.014253</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.813424</td>\n",
       "      <td>0.025651</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.837464</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.822951</td>\n",
       "      <td>0.015170</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.833170</td>\n",
       "      <td>0.013815</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.812256</td>\n",
       "      <td>0.027872</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mean       std  count\n",
       "gene_name model num_genes                           \n",
       "RPL5      DT    16         0.760771  0.025832      5\n",
       "                32         0.773893  0.023490      5\n",
       "                64         0.778796  0.033080      5\n",
       "                128        0.775419  0.026961      5\n",
       "                256        0.782436  0.034340      5\n",
       "                512        0.796946  0.013723      5\n",
       "          LR-L1 16         0.815425  0.014253      5\n",
       "                32         0.813424  0.025651      5\n",
       "                64         0.837464  0.012902      5\n",
       "                128        0.822951  0.015170      5\n",
       "                256        0.833170  0.013815      5\n",
       "                512        0.812256  0.027872      5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = results[\"df\"].groupby(['gene_name', 'model','num_genes'])['auc'].agg(['mean','std', 'count'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results[\"df\"].groupby(['gene_name', 'model','num_genes'])['auc'].mean().groupby([\"model\"]).plot(legend=True, sharex=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCciShTHGmIAsWRhjjAno/wH5yo5I1tVEWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4469524b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for model in results[\"df\"][\"model\"].unique():\n",
    "    index = results[\"df\"].groupby(['model','num_genes'])['auc'].mean()[model].index\n",
    "    mean = results[\"df\"].groupby(['model','num_genes'])['auc'].mean()[model]\n",
    "    stderr = results[\"df\"].groupby(['model','num_genes'])['auc'].std()[model]\n",
    "    plt.errorbar(index, mean,label=model, xerr=0, yerr=stderr)\n",
    "\n",
    "\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(sorted(results[\"df\"][\"num_genes\"].unique()))\n",
    "formatter = matplotlib.ticker.ScalarFormatter()\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#results_df = pd.DataFrame(columns=['model', 'num_genes', 'gene_name', 'auc', 'std'])\n",
    "#results_df = results_df.append(data=pd.DataFrame(pd.DataFrame(data={'model':\"LR\", 'num_genes': 10.0, 'gene_name': \"RPL5\", 'auc':0.57, 'std': 0.01}, index=[0]))\n",
    "len([\"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\"])\n",
    "len([10.0, 10.0, 10.0, 10.0, 510.0, 510.0, 510.0, 510.0, 1010.0, 1010.0, 1010.0, 1010.0, 1510.0, 1510.0, 1510.0, 1510.0, 2010.0, 2010.0, 2010.0, 2010.0, 2510.0, 2510.0, 2510.0, 2510.0, 3010.0, 3010.0, 3010.0, 3010.0])\n",
    "len([0.57, 0.56, 0.55, 0.64, 0.81, 0.83, .79, .94, .81, .80, .78, .94, .80, .74, .77, .93, .78, .79, .78, .92, .77, .77, .76, .92, .76, .71, .76, .92])\n",
    "len([0.01, 0.04, 0.03, 0.01, 0.02, 0.01, 0.03, 0.00, 0.01, 0.03, 0.02, 0.01, 0.03, 0.12, 0.02, 0.00, 0.03, 0.02, 0.03, 0.01, 0.02, 0.05, 0.04, 0.01, 0.02 ,0.11, 0.02, 0.00])\n",
    "results_df = pd.DataFrame(data={'model':[\"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\"],\n",
    "                   'num_genes': [10.0, 10.0, 10.0, 10.0, 510.0, 510.0, 510.0, 510.0, 1010.0, 1010.0, 1010.0, 1010.0, 1510.0, 1510.0, 1510.0, 1510.0, 2010.0, 2010.0, 2010.0, 2010.0, 2510.0, 2510.0, 2510.0, 2510.0, 3010.0, 3010.0, 3010.0, 3010.0],\n",
    "#                  'gene_name': [\"RPL5\", \"RPL5\"],\n",
    "                   'auc': [0.57, 0.56, 0.55, 0.64, 0.81, 0.83, .79, .94, .81, .80, .78, .94, .80, .74, .77, .93, .78, .79, .78, .92, .77, .77, .76, .92, .76, .71, .76, .92],\n",
    "                   'std': [0.01, 0.04, 0.03, 0.01, 0.02, 0.01, 0.03, 0.00, 0.01, 0.03, 0.02, 0.01, 0.03, 0.12, 0.02, 0.00, 0.03, 0.02, 0.03, 0.01, 0.02, 0.05, 0.04, 0.01, 0.02 ,0.11, 0.02, 0.00]}, index=range(0, 28))\n",
    "plt.figure()\n",
    "titles = []\n",
    "for model in [\n",
    "    {'key': 'LR', 'method': lr},\n",
    "    {'key': 'MLP', 'method': mlp},\n",
    "    {'key': 'Decision Tree', 'method': decision_tree},\n",
    "    {'key': 'CGN_3_layer_64_channel_emb_32_dropout', 'method': cgn_loop, 'num_channel': 64, 'num_layer': 3, 'add_emb': 32, 'use_gate': False, 'dropout': True, 'cuda': True},\n",
    "    ]:\n",
    "    temp_results = results_df.loc[results_df['model'] == model['key']].reset_index(drop=True)\n",
    "    lines.append(plt.errorbar(temp_results.index, temp_results['auc'], xerr=0, yerr=temp_results['std'])[0])\n",
    "    titles.append(model['key'])\n",
    "    plt.xticks(list(temp_results.index), temp_results['num_genes'], rotation=70)\n",
    "width = 0.2\n",
    "plt.title(\"Inferring the value of RPL5 with varying numbers of genes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"# genes\")\n",
    "plt.legend(lines, titles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results of adding Nodes\n",
    "plt.figure()\n",
    "\n",
    "#full_results.loc[full_results['samples'] == 100]\n",
    "\n",
    "line1 = plt.errorbar(lr_results.index, lr_results['auc'], xerr=0, yerr=lr_results['std'])\n",
    "line2 = plt.errorbar(cgn_results.index, cgn_results['auc'], xerr=0, yerr=cgn_results['std'])\n",
    "\n",
    "width = 0.2\n",
    "plt.xticks(list(lr_results.iloc[::5, :].index), lr_results.iloc[::5, :]['num_genes'], rotation=70)\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.legend((line1[0], line2[0]), ('LR', \"CGN\"), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict a gene from a growing number of Nodes\n",
    "lr_results = pd.DataFrame([])\n",
    "mlp_results = pd.DataFrame([])\n",
    "cgn_results = pd.DataFrame([])\n",
    "gene = \"RPL5\"\n",
    "max_samples = 200\n",
    "reload(data)\n",
    "reload(models)\n",
    "tcgatissue = data.gene_datasets.TCGATissue(data_dir='./genomics/TCGA/', data_file='TCGA_tissue_ppi.hdf5')\n",
    "\n",
    "for num_samples in range(10, max_samples, 20):\n",
    "    lr_row = infer_gene(lr, tcgatissue, \"RPL5\", train_size=num_samples, test_size=200, trials=3, penalty=True)\n",
    "    lr_results = lr_results.append(lr_row).reset_index(drop=True)\n",
    "    lr_results.loc[lr_results.index[-1], 'num_samples'] = num_samples\n",
    "    cgn_row = infer_gene(cgn, tcgatissue, \"RPL5\", train_size=num_samples, test_size=200, trials=3, penalty=True)\n",
    "    cgn_results = cgn_results.append(cgn_row).reset_index(drop=True)\n",
    "    cgn_results.loc[lr_results.index[-1], 'num_samples'] = num_samples\n",
    "    print num_genes\n",
    "    print cgn_results\n",
    "    print lr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results of adding Nodes\n",
    "plt.figure()\n",
    "\n",
    "#full_results.loc[full_results['samples'] == 100]\n",
    "\n",
    "line1 = plt.errorbar(lr_results.index, lr_results['auc'], xerr=0, yerr=lr_results['std'])\n",
    "line2 = plt.errorbar(cgn_results.index, cgn_results['auc'], xerr=0, yerr=cgn_results['std'])\n",
    "\n",
    "width = 0.2\n",
    "plt.xticks(list(lr_results.iloc[::5, :].index), lr_results.iloc[::5, :]['num_samples'], rotation=70)\n",
    "plt.title(\"Gene Inference with varying numbers of samples\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of samples\")\n",
    "plt.legend((line1[0], line2[0]), ('LR', \"CGN\"), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
