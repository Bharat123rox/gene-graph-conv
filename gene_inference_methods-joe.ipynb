{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from itertools import repeat\n",
    "import data, data.gene_datasets\n",
    "import sklearn, sklearn.model_selection, sklearn.metrics, sklearn.linear_model, sklearn.neural_network, sklearn.tree\n",
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import gene_inference\n",
    "#from gene_inference.infer_genes import infer_all_genes, sample_neighbors\n",
    "import models, models.graphLayer\n",
    "from models.models import CGN\n",
    "import data, data.gene_datasets\n",
    "from data.graph import Graph\n",
    "from data.utils import split_dataset\n",
    "import optimization\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from analysis.metrics import record_metrics_for_epoch\n",
    "import analysis\n",
    "reload(analysis.metrics)\n",
    "reload(gene_inference);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting one-hot labels to integers\n"
     ]
    }
   ],
   "source": [
    "#tcgatissue = data.gene_datasets.TCGATissue(data_dir='./genomics/TCGA/', data_file='TCGA_tissue_ppi.hdf5')\n",
    "tcgatissue = data.gene_datasets.TCGATissue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "opt = Object()\n",
    "opt.seed = 0\n",
    "opt.nb_class = None\n",
    "opt.nb_examples = None\n",
    "opt.nb_nodes = None\n",
    "opt.graph = \"pathway\"\n",
    "opt.dataset = tcgatissue\n",
    "opt.add_self = True\n",
    "opt.norm_adj = True\n",
    "opt.add_connectivity = False\n",
    "opt.num_layer = 1\n",
    "opt.cuda = True\n",
    "opt.pool_graph = \"ignore\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "path = \"/data/lisa/data/genomics/graph/pancan-tissue-graph.hdf5\"\n",
    "graph.load_graph(path)\n",
    "#graph.intersection_with(tcgatissue)\n",
    "g = nx.from_numpy_matrix(graph.adj)\n",
    "mapping = dict(zip(range(0, len(tcgatissue.df.columns)), tcgatissue.df.columns))\n",
    "g = nx.relabel_nodes(g, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_neighbors(g, gene, num_neighbors, include_self=True):\n",
    "    results = set([])\n",
    "    if include_self:\n",
    "        results = set([gene])\n",
    "    all_nodes = set(g.nodes)\n",
    "    first_degree = set(g.neighbors(gene))\n",
    "    second_degree = set()\n",
    "    for x in g.neighbors(gene):\n",
    "        second_degree = second_degree.union(set(g.neighbors(x)))\n",
    "    while len(results) < num_neighbors:\n",
    "        if len(first_degree) - len(results) > 0:\n",
    "            unique = first_degree - results\n",
    "            results.add(np.random.choice(list(unique)))\n",
    "        elif len(second_degree) - len(results) > 0:\n",
    "            unique = second_degree - results\n",
    "            results.add(np.random.choice(list(unique)))\n",
    "        else:\n",
    "            unique = all_nodes - results\n",
    "            results.add(np.random.choice(list(unique)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample_neighbors(g, \"RPL5\", 5, include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn, sklearn.model_selection, sklearn.metrics, sklearn.linear_model, sklearn.neural_network, sklearn.tree\n",
    "import numpy as np\n",
    "\n",
    "class Method:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class SkLearn(Method):\n",
    "    \n",
    "    def __init__(self, model, penalty=False):\n",
    "        self.model = model\n",
    "        self.penalty = penalty\n",
    "        \n",
    "    def loop(self, dataset, seed, train_size, test_size, adj=None):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset.df, dataset.labels, stratify=dataset.labels, train_size=train_size, test_size=test_size, random_state=seed)\n",
    "\n",
    "        if self.model == \"LR\":\n",
    "            model = sklearn.linear_model.LogisticRegression()\n",
    "            if self.penalty:\n",
    "                model = sklearn.linear_model.LogisticRegression(penalty='l1', tol=0.0001)\n",
    "        elif self.model == \"DT\":\n",
    "            model = sklearn.tree.DecisionTreeClassifier()\n",
    "        elif self.model == \"MLP\":\n",
    "            model = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(32,3), learning_rate_init=0.001, early_stopping=False,  max_iter=1000)\n",
    "        else:\n",
    "            print \"incorrect label\"\n",
    "        \n",
    "        model = model.fit(X_train, y_train)\n",
    "        return sklearn.metrics.roc_auc_score(y_test, model.predict(X_test))\n",
    "\n",
    "\n",
    "class PyTorch(Method):    \n",
    "    \n",
    "    def __init__(self, model, num_epochs=100, num_channel=64, num_layer=3, add_emb=32, use_gate=False, dropout=True, cuda=True):\n",
    "        self.model = model\n",
    "        self.batch_size = 10\n",
    "        self.num_channel = num_channel\n",
    "        self.num_layer = num_layer\n",
    "        self.add_emb = add_emb\n",
    "        self.use_gate = use_gate\n",
    "        self.dropout = dropout\n",
    "        self.cuda = cuda\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def loop(self, dataset, seed, train_size, test_size, adj=None):\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset.df, dataset.labels, stratify=dataset.labels, train_size=train_size, test_size=test_size, random_state=seed)\n",
    "    \n",
    "        #split train into valid and train\n",
    "        local_X_train, local_X_valid, local_y_train, local_y_valid = sklearn.model_selection.train_test_split(X_train, y_train, stratify=y_train, train_size=0.60, random_state=seed)\n",
    "    \n",
    "    \n",
    "        local_X_train = torch.FloatTensor(np.expand_dims(local_X_train, axis=2))\n",
    "        local_X_valid = torch.FloatTensor(np.expand_dims(local_X_valid, axis=2))\n",
    "        X_test = torch.FloatTensor(np.expand_dims(X_test, axis=2))\n",
    "        \n",
    "        local_y_train = torch.FloatTensor(local_y_train)\n",
    "\n",
    "        criterion = optimization.get_criterion(dataset)\n",
    "        \n",
    "        patience = 20\n",
    "        opt.num_layer = self.num_layer\n",
    "        adj_transform, aggregate_function = models.graphLayer.get_transform(opt, adj)\n",
    "        \n",
    "        if self.model == \"CGN\":\n",
    "            model = models.models.CGN(\n",
    "                    nb_nodes=len(dataset.df.columns), \n",
    "                    input_dim=1,\n",
    "                    channels=[self.num_channel] * self.num_layer,\n",
    "                    adj=adj,\n",
    "                    out_dim=2,\n",
    "                    on_cuda=self.cuda,\n",
    "                    add_emb=self.add_emb,\n",
    "                    transform_adj=adj_transform,\n",
    "                    aggregate_adj=aggregate_function,\n",
    "                    use_gate=self.use_gate,\n",
    "                    dropout=self.dropout,\n",
    "                    )\n",
    "        elif self.model == \"MLP\":\n",
    "            model = models.models.MLP(\n",
    "                    len(dataset.df.columns), \n",
    "                    channels=[self.num_channel] * self.num_layer, \n",
    "                    out_dim=2, \n",
    "                    on_cuda=self.cuda, \n",
    "                    dropout=self.dropout)\n",
    "            \n",
    "            \n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            model.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        max_valid = 0\n",
    "        for t in range(0, self.num_epochs):\n",
    "            start_timer = time.time()\n",
    "            \n",
    "            if self.cuda:\n",
    "                model.cuda()\n",
    "                model.on_cuda = True\n",
    "            \n",
    "            for base_x in range(0,local_X_train.shape[0], self.batch_size):\n",
    "                inputs, labels = local_X_train[base_x:base_x+self.batch_size], local_y_train[base_x:base_x+self.batch_size]\n",
    "\n",
    "                inputs = Variable(inputs, requires_grad=False).float()\n",
    "                if self.cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                model.train()\n",
    "                y_pred = model(inputs)\n",
    "\n",
    "                # Compute and print loss\n",
    "                crit_loss = optimization.compute_loss(criterion, y_pred, labels)\n",
    "                total_loss = crit_loss\n",
    "\n",
    "                # Zero gradients, perform a backward pass, and update the weights.\n",
    "                optimizer.zero_grad()\n",
    "                crit_loss.backward()\n",
    "                optimizer.step()\n",
    "                model.eval()\n",
    "            time_this_epoch = time.time() - start_timer\n",
    "            \n",
    "            \n",
    "            auc = {}\n",
    "            if self.cuda:\n",
    "                model.cpu()\n",
    "                model.on_cuda = False\n",
    "\n",
    "            auc['train'] = sklearn.metrics.roc_auc_score(local_y_train.numpy(), model(Variable(local_X_train.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            auc['valid'] = sklearn.metrics.roc_auc_score(local_y_valid, model(Variable(local_X_valid.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            auc['test'] = sklearn.metrics.roc_auc_score(y_test, model(Variable(X_test.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            \n",
    "            #print auc['test']\n",
    "            summary = [ t, crit_loss.data[0], auc['train'], auc['valid'], time_this_epoch ]\n",
    "            summary = \"epoch {}, cross_loss: {:.03f}, auc_train: {:0.3f}, auc_valid:{:0.3f}, time: {:.02f} sec\".format(*summary)\n",
    "            print summary\n",
    "\n",
    "            patience = patience - 1\n",
    "            if patience == 0:\n",
    "                return auc['test']\n",
    "                break\n",
    "            if max_valid < auc['valid']:\n",
    "                max_valid = auc['valid']\n",
    "            if max_valid > auc['valid'] and t > 15:\n",
    "                #scores.append(auc['test']) \n",
    "                #print \"returning\", auc['test']\n",
    "                return auc['test']\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def method_comparison(results, dataset, models, gene, max_genes, trials, train_size, test_size):\n",
    "    \n",
    "    dataset = data.gene_datasets.TCGATissue()\n",
    "    dataset.df = dataset.df - dataset.df.mean()\n",
    "    \n",
    "    mean = dataset.df[gene].mean()\n",
    "    dataset.labels = [1 if x > mean else 0 for x in dataset.df[gene]]\n",
    "    full_df = dataset.df.copy(deep=True)\n",
    "    \n",
    "    print \"Max ex \", int(np.log2(max_genes))+1\n",
    "    for ex in range(4, int(np.log2(max_genes))+1):\n",
    "        \n",
    "        num_genes = 2**ex\n",
    "        num_genes = np.min([num_genes, tcgatissue.df.shape[1]])\n",
    "        print ex, num_genes\n",
    "        \n",
    "        neighbors = sample_neighbors(g, gene, num_genes, include_self=False)\n",
    "        print \"neighbors\", len(neighbors)\n",
    "        \n",
    "        if gene in neighbors:\n",
    "            neighbors.remove(gene)\n",
    "\n",
    "        dataset.df = dataset.df[list(neighbors)]\n",
    "        dataset.data = dataset.df.as_matrix()\n",
    "        \n",
    "        neighborhood = np.asarray(nx.to_numpy_matrix(nx.Graph(g.subgraph(neighbors))))\n",
    "        \n",
    "        for model in models:\n",
    "            for seed in range(trials):\n",
    "            \n",
    "                #have we already done it?\n",
    "                already_done = results[\"df\"][(results[\"df\"].gene_name == gene) & \n",
    "                                             (results[\"df\"].model == model['key']) &\n",
    "                                             (results[\"df\"].num_genes == num_genes) &\n",
    "                                             (results[\"df\"].seed == seed) &\n",
    "                                             (results[\"df\"].train_size == train_size)].shape[0] > 0\n",
    "\n",
    "                if already_done:\n",
    "                    print \"already done:\", model['key'], num_genes, seed\n",
    "                    continue\n",
    "                print \"doing:\", model['key'], num_genes, seed\n",
    "\n",
    "                result = model['method'].loop(dataset=dataset, seed=seed, train_size=train_size, test_size=test_size, adj=neighborhood)\n",
    "\n",
    "                experiment = {\"gene_name\": gene,\n",
    "                        \"model\": model['key'],\n",
    "                        \"num_genes\": num_genes, \n",
    "                        \"seed\":seed,\n",
    "                        \"train_size\": train_size,\n",
    "                        \"auc\":result\n",
    "                        }\n",
    "\n",
    "                results[\"df\"] = results[\"df\"].append(experiment, ignore_index=True)\n",
    "                pickle.dump(results, open(\"results-temp.pkl\", \"wb\"))\n",
    "        dataset.df = full_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data\n",
    "reload(data)\n",
    "#reload(models.models)\n",
    "reload(gene_inference)\n",
    "#reload(gene_inference.models)\n",
    "reload(analysis.metrics)\n",
    "import pickle\n",
    "\n",
    "m = [\n",
    "    {'key': 'LR-L1', 'method': SkLearn(\"LR\", penalty=True)},\n",
    "#    {'key': 'MLP', 'method': mlp},\n",
    "    {'key': 'DT', 'method': SkLearn(\"DT\")},\n",
    "#   {'key': 'CGN_3_layer_64_channel_emb_32_dropout', 'method': PyTorch(\"CGN\")},\n",
    "#   {'key': 'MLP-dropout', 'method': PyTorch(\"MLP\")},\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results = {\"df\": pd.DataFrame(columns=['auc','gene_name', 'model', 'num_genes', 'seed', 'train_size'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#results = pickle.load(open(\"results-temp.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting one-hot labels to integers\n",
      "Max ex  14\n",
      "4 16\n",
      "neighbors 16\n",
      "already done: LR-L1 16 0\n",
      "already done: LR-L1 16 1\n",
      "already done: DT 16 0\n",
      "already done: DT 16 1\n",
      "5 32\n",
      "neighbors 32\n",
      "already done: LR-L1 32 0\n",
      "already done: LR-L1 32 1\n",
      "already done: DT 32 0\n",
      "already done: DT 32 1\n",
      "6 64\n",
      "neighbors 64\n",
      "already done: LR-L1 64 0\n",
      "already done: LR-L1 64 1\n",
      "already done: DT 64 0\n",
      "already done: DT 64 1\n",
      "7 128\n",
      "neighbors 128\n",
      "already done: LR-L1 128 0\n",
      "already done: LR-L1 128 1\n",
      "already done: DT 128 0\n",
      "already done: DT 128 1\n",
      "8 256\n",
      "neighbors 256\n",
      "already done: LR-L1 256 0\n",
      "already done: LR-L1 256 1\n",
      "already done: DT 256 0\n",
      "already done: DT 256 1\n",
      "9 512\n",
      "neighbors 512\n",
      "already done: LR-L1 512 0\n",
      "already done: LR-L1 512 1\n",
      "already done: DT 512 0\n",
      "already done: DT 512 1\n",
      "10 1024\n",
      "neighbors 1024\n",
      "already done: LR-L1 1024 0\n",
      "already done: LR-L1 1024 1\n",
      "already done: DT 1024 0\n",
      "already done: DT 1024 1\n",
      "11 2048\n",
      "neighbors 2048\n",
      "doing: LR-L1 2048 0\n",
      "doing: LR-L1 2048 1\n",
      "doing: DT 2048 0\n",
      "doing: DT 2048 1\n",
      "12 4096\n",
      "neighbors 4096\n",
      "doing: LR-L1 4096 0\n",
      "doing: LR-L1 4096 1\n",
      "doing: DT 4096 0\n",
      "doing: DT 4096 1\n",
      "13 8192\n",
      "neighbors 8192\n",
      "doing: LR-L1 8192 0\n",
      "doing: LR-L1 8192 1\n",
      "doing: DT 8192 0\n",
      "doing: DT 8192 1\n"
     ]
    }
   ],
   "source": [
    "method_comparison(results, tcgatissue, m, gene=\"RPL5\", max_genes=9000, trials=2, train_size=100, test_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(results, open(\"results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results = pickle.load(open(\"results.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>gene_name</th>\n",
       "      <th>model</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>seed</th>\n",
       "      <th>train_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800461</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.822363</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.832213</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.731107</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.760795</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.801509</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.773086</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.812424</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.816969</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.735812</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.768829</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.791539</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.826091</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.850818</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.835894</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.740982</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.806494</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.760899</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.811384</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.814681</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.847833</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.753697</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.784865</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.787122</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.820146</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.854851</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.837862</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.785793</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.811384</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.727018</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.908985</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.931699</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.919719</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.935936</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.920852</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.938444</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.911405</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.927185</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.912318</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.907724</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.734060</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.827068</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.792899</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>1024</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.801317</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.809919</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.747687</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.768581</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.774262</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>1024</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.801245</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.765940</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.905144</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.907344</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>1024</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.881510</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.881830</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>MLP-dropout</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.850946</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>MLP-dropout</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.902715</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>MLP-dropout</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          auc gene_name                                  model num_genes seed  \\\n",
       "0    0.800461      RPL5                                  LR-L1        16    0   \n",
       "1    0.822363      RPL5                                  LR-L1        16    1   \n",
       "2    0.832213      RPL5                                  LR-L1        16    2   \n",
       "3    0.731107      RPL5                                     DT        16    0   \n",
       "4    0.760795      RPL5                                     DT        16    1   \n",
       "5    0.801509      RPL5                                     DT        16    2   \n",
       "6    0.773086      RPL5                                  LR-L1        32    0   \n",
       "7    0.812424      RPL5                                  LR-L1        32    1   \n",
       "8    0.816969      RPL5                                  LR-L1        32    2   \n",
       "9    0.735812      RPL5                                     DT        32    0   \n",
       "10   0.768829      RPL5                                     DT        32    1   \n",
       "11   0.791539      RPL5                                     DT        32    2   \n",
       "12   0.826091      RPL5                                  LR-L1        64    0   \n",
       "13   0.850818      RPL5                                  LR-L1        64    1   \n",
       "14   0.835894      RPL5                                  LR-L1        64    2   \n",
       "15   0.740982      RPL5                                     DT        64    0   \n",
       "16   0.806494      RPL5                                     DT        64    1   \n",
       "17   0.760899      RPL5                                     DT        64    2   \n",
       "18   0.811384      RPL5                                  LR-L1       128    0   \n",
       "19   0.814681      RPL5                                  LR-L1       128    1   \n",
       "20   0.847833      RPL5                                  LR-L1       128    2   \n",
       "21   0.753697      RPL5                                     DT       128    0   \n",
       "22   0.784865      RPL5                                     DT       128    1   \n",
       "23   0.787122      RPL5                                     DT       128    2   \n",
       "24   0.820146      RPL5                                  LR-L1       256    0   \n",
       "25   0.854851      RPL5                                  LR-L1       256    1   \n",
       "26   0.837862      RPL5                                  LR-L1       256    2   \n",
       "27   0.785793      RPL5                                     DT       256    0   \n",
       "28   0.811384      RPL5                                     DT       256    1   \n",
       "29   0.727018      RPL5                                     DT       256    2   \n",
       "..        ...       ...                                    ...       ...  ...   \n",
       "77        NaN      RPL5  CGN_3_layer_64_channel_emb_32_dropout        64    4   \n",
       "78   0.908985      RPL5  CGN_3_layer_64_channel_emb_32_dropout       128    1   \n",
       "79   0.931699      RPL5  CGN_3_layer_64_channel_emb_32_dropout       128    2   \n",
       "80        NaN      RPL5  CGN_3_layer_64_channel_emb_32_dropout       128    3   \n",
       "81        NaN      RPL5  CGN_3_layer_64_channel_emb_32_dropout       128    4   \n",
       "82   0.919719      RPL5  CGN_3_layer_64_channel_emb_32_dropout       256    1   \n",
       "83   0.935936      RPL5  CGN_3_layer_64_channel_emb_32_dropout       256    2   \n",
       "84   0.920852      RPL5  CGN_3_layer_64_channel_emb_32_dropout       256    3   \n",
       "85   0.938444      RPL5  CGN_3_layer_64_channel_emb_32_dropout       256    4   \n",
       "86   0.911405      RPL5  CGN_3_layer_64_channel_emb_32_dropout       512    1   \n",
       "87   0.927185      RPL5  CGN_3_layer_64_channel_emb_32_dropout       512    2   \n",
       "88   0.912318      RPL5  CGN_3_layer_64_channel_emb_32_dropout       512    3   \n",
       "89   0.907724      RPL5  CGN_3_layer_64_channel_emb_32_dropout       512    4   \n",
       "90   0.734060      RPL5                                  LR-L1      1024    0   \n",
       "91   0.827068      RPL5                                  LR-L1      1024    1   \n",
       "92   0.792899      RPL5                                  LR-L1      1024    2   \n",
       "93   0.801317      RPL5                                  LR-L1      1024    3   \n",
       "94   0.809919      RPL5                                  LR-L1      1024    4   \n",
       "95   0.747687      RPL5                                     DT      1024    0   \n",
       "96   0.768581      RPL5                                     DT      1024    1   \n",
       "97   0.774262      RPL5                                     DT      1024    2   \n",
       "98   0.801245      RPL5                                     DT      1024    3   \n",
       "99   0.765940      RPL5                                     DT      1024    4   \n",
       "100       NaN      RPL5  CGN_3_layer_64_channel_emb_32_dropout      1024    0   \n",
       "101  0.905144      RPL5  CGN_3_layer_64_channel_emb_32_dropout      1024    1   \n",
       "102  0.907344      RPL5  CGN_3_layer_64_channel_emb_32_dropout      1024    2   \n",
       "103  0.881510      RPL5  CGN_3_layer_64_channel_emb_32_dropout      1024    3   \n",
       "104  0.881830      RPL5                            MLP-dropout        16    0   \n",
       "105  0.850946      RPL5                            MLP-dropout        32    0   \n",
       "106  0.902715      RPL5                            MLP-dropout        64    0   \n",
       "\n",
       "    train_size  \n",
       "0          100  \n",
       "1          100  \n",
       "2          100  \n",
       "3          100  \n",
       "4          100  \n",
       "5          100  \n",
       "6          100  \n",
       "7          100  \n",
       "8          100  \n",
       "9          100  \n",
       "10         100  \n",
       "11         100  \n",
       "12         100  \n",
       "13         100  \n",
       "14         100  \n",
       "15         100  \n",
       "16         100  \n",
       "17         100  \n",
       "18         100  \n",
       "19         100  \n",
       "20         100  \n",
       "21         100  \n",
       "22         100  \n",
       "23         100  \n",
       "24         100  \n",
       "25         100  \n",
       "26         100  \n",
       "27         100  \n",
       "28         100  \n",
       "29         100  \n",
       "..         ...  \n",
       "77         100  \n",
       "78         100  \n",
       "79         100  \n",
       "80         100  \n",
       "81         100  \n",
       "82         100  \n",
       "83         100  \n",
       "84         100  \n",
       "85         100  \n",
       "86         100  \n",
       "87         100  \n",
       "88         100  \n",
       "89         100  \n",
       "90         100  \n",
       "91         100  \n",
       "92         100  \n",
       "93         100  \n",
       "94         100  \n",
       "95         100  \n",
       "96         100  \n",
       "97         100  \n",
       "98         100  \n",
       "99         100  \n",
       "100        100  \n",
       "101        100  \n",
       "102        100  \n",
       "103        100  \n",
       "104        100  \n",
       "105        100  \n",
       "106        100  \n",
       "\n",
       "[107 rows x 6 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene_name</th>\n",
       "      <th>model</th>\n",
       "      <th>num_genes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"38\" valign=\"top\">RPL5</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">CGN_3_layer_64_channel_emb_32_dropout</th>\n",
       "      <th>16</th>\n",
       "      <td>0.884154</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.889814</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.910562</td>\n",
       "      <td>0.032372</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.915712</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.924305</td>\n",
       "      <td>0.013063</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.914658</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.898535</td>\n",
       "      <td>0.011743</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>0.845128</td>\n",
       "      <td>0.036117</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">DT</th>\n",
       "      <th>16</th>\n",
       "      <td>0.760771</td>\n",
       "      <td>0.025832</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.773893</td>\n",
       "      <td>0.023490</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.778796</td>\n",
       "      <td>0.033080</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.775419</td>\n",
       "      <td>0.026961</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.782436</td>\n",
       "      <td>0.034340</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.796946</td>\n",
       "      <td>0.013723</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.771543</td>\n",
       "      <td>0.019358</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>0.756102</td>\n",
       "      <td>0.033458</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>0.754941</td>\n",
       "      <td>0.044570</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>0.753009</td>\n",
       "      <td>0.054897</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">LR-L1</th>\n",
       "      <th>16</th>\n",
       "      <td>0.815425</td>\n",
       "      <td>0.014253</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.813424</td>\n",
       "      <td>0.025651</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.837464</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.822951</td>\n",
       "      <td>0.015170</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.833170</td>\n",
       "      <td>0.013815</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.812256</td>\n",
       "      <td>0.027872</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.793053</td>\n",
       "      <td>0.035320</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>0.754381</td>\n",
       "      <td>0.061704</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>0.735836</td>\n",
       "      <td>0.043739</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">MLP-dropout</th>\n",
       "      <th>16</th>\n",
       "      <td>0.908190</td>\n",
       "      <td>0.007624</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.880014</td>\n",
       "      <td>0.018108</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.902317</td>\n",
       "      <td>0.014856</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.913143</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.925052</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.913289</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.892497</td>\n",
       "      <td>0.010847</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>0.852810</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>0.818636</td>\n",
       "      <td>0.031218</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>0.792722</td>\n",
       "      <td>0.030958</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               mean       std  \\\n",
       "gene_name model                                 num_genes                       \n",
       "RPL5      CGN_3_layer_64_channel_emb_32_dropout 16         0.884154  0.008160   \n",
       "                                                32         0.889814  0.030178   \n",
       "                                                64         0.910562  0.032372   \n",
       "                                                128        0.915712  0.013903   \n",
       "                                                256        0.924305  0.013063   \n",
       "                                                512        0.914658  0.008584   \n",
       "                                                1024       0.898535  0.011743   \n",
       "                                                2048       0.845128  0.036117   \n",
       "          DT                                    16         0.760771  0.025832   \n",
       "                                                32         0.773893  0.023490   \n",
       "                                                64         0.778796  0.033080   \n",
       "                                                128        0.775419  0.026961   \n",
       "                                                256        0.782436  0.034340   \n",
       "                                                512        0.796946  0.013723   \n",
       "                                                1024       0.771543  0.019358   \n",
       "                                                2048       0.756102  0.033458   \n",
       "                                                4096       0.754941  0.044570   \n",
       "                                                8192       0.753009  0.054897   \n",
       "          LR-L1                                 16         0.815425  0.014253   \n",
       "                                                32         0.813424  0.025651   \n",
       "                                                64         0.837464  0.012902   \n",
       "                                                128        0.822951  0.015170   \n",
       "                                                256        0.833170  0.013815   \n",
       "                                                512        0.812256  0.027872   \n",
       "                                                1024       0.793053  0.035320   \n",
       "                                                2048       0.754381  0.061704   \n",
       "                                                4096       0.735836  0.043739   \n",
       "                                                8192       0.721357  0.004849   \n",
       "          MLP-dropout                           16         0.908190  0.007624   \n",
       "                                                32         0.880014  0.018108   \n",
       "                                                64         0.902317  0.014856   \n",
       "                                                128        0.913143  0.008110   \n",
       "                                                256        0.925052  0.009755   \n",
       "                                                512        0.913289  0.010302   \n",
       "                                                1024       0.892497  0.010847   \n",
       "                                                2048       0.852810  0.016475   \n",
       "                                                4096       0.818636  0.031218   \n",
       "                                                8192       0.792722  0.030958   \n",
       "\n",
       "                                                           count  \n",
       "gene_name model                                 num_genes         \n",
       "RPL5      CGN_3_layer_64_channel_emb_32_dropout 16             5  \n",
       "                                                32             3  \n",
       "                                                64             4  \n",
       "                                                128            3  \n",
       "                                                256            5  \n",
       "                                                512            4  \n",
       "                                                1024           4  \n",
       "                                                2048           2  \n",
       "          DT                                    16             5  \n",
       "                                                32             5  \n",
       "                                                64             5  \n",
       "                                                128            5  \n",
       "                                                256            5  \n",
       "                                                512            5  \n",
       "                                                1024           5  \n",
       "                                                2048           2  \n",
       "                                                4096           2  \n",
       "                                                8192           2  \n",
       "          LR-L1                                 16             5  \n",
       "                                                32             5  \n",
       "                                                64             5  \n",
       "                                                128            5  \n",
       "                                                256            5  \n",
       "                                                512            5  \n",
       "                                                1024           5  \n",
       "                                                2048           2  \n",
       "                                                4096           2  \n",
       "                                                8192           2  \n",
       "          MLP-dropout                           16             5  \n",
       "                                                32             5  \n",
       "                                                64             5  \n",
       "                                                128            5  \n",
       "                                                256            5  \n",
       "                                                512            5  \n",
       "                                                1024           5  \n",
       "                                                2048           5  \n",
       "                                                4096           5  \n",
       "                                                8192           3  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = results[\"df\"].groupby(['gene_name', 'model','num_genes'])['auc'].agg(['mean','std', 'count'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results[\"df\"].groupby(['gene_name', 'model','num_genes'])['auc'].mean().groupby([\"model\"]).plot(legend=True, sharex=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc2d402ac90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for model in results[\"df\"][\"model\"].unique():\n",
    "    index = results[\"df\"].groupby(['model','num_genes'])['auc'].mean()[model].index\n",
    "    mean = results[\"df\"].groupby(['model','num_genes'])['auc'].mean()[model]\n",
    "    stderr = results[\"df\"].groupby(['model','num_genes'])['auc'].std()[model]\n",
    "    plt.errorbar(index, mean,label=model, xerr=0, yerr=stderr)\n",
    "\n",
    "\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(sorted(results[\"df\"][\"num_genes\"].unique()))\n",
    "formatter = matplotlib.ticker.ScalarFormatter()\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#results_df = pd.DataFrame(columns=['model', 'num_genes', 'gene_name', 'auc', 'std'])\n",
    "#results_df = results_df.append(data=pd.DataFrame(pd.DataFrame(data={'model':\"LR\", 'num_genes': 10.0, 'gene_name': \"RPL5\", 'auc':0.57, 'std': 0.01}, index=[0]))\n",
    "len([\"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\"])\n",
    "len([10.0, 10.0, 10.0, 10.0, 510.0, 510.0, 510.0, 510.0, 1010.0, 1010.0, 1010.0, 1010.0, 1510.0, 1510.0, 1510.0, 1510.0, 2010.0, 2010.0, 2010.0, 2010.0, 2510.0, 2510.0, 2510.0, 2510.0, 3010.0, 3010.0, 3010.0, 3010.0])\n",
    "len([0.57, 0.56, 0.55, 0.64, 0.81, 0.83, .79, .94, .81, .80, .78, .94, .80, .74, .77, .93, .78, .79, .78, .92, .77, .77, .76, .92, .76, .71, .76, .92])\n",
    "len([0.01, 0.04, 0.03, 0.01, 0.02, 0.01, 0.03, 0.00, 0.01, 0.03, 0.02, 0.01, 0.03, 0.12, 0.02, 0.00, 0.03, 0.02, 0.03, 0.01, 0.02, 0.05, 0.04, 0.01, 0.02 ,0.11, 0.02, 0.00])\n",
    "results_df = pd.DataFrame(data={'model':[\"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\"],\n",
    "                   'num_genes': [10.0, 10.0, 10.0, 10.0, 510.0, 510.0, 510.0, 510.0, 1010.0, 1010.0, 1010.0, 1010.0, 1510.0, 1510.0, 1510.0, 1510.0, 2010.0, 2010.0, 2010.0, 2010.0, 2510.0, 2510.0, 2510.0, 2510.0, 3010.0, 3010.0, 3010.0, 3010.0],\n",
    "#                  'gene_name': [\"RPL5\", \"RPL5\"],\n",
    "                   'auc': [0.57, 0.56, 0.55, 0.64, 0.81, 0.83, .79, .94, .81, .80, .78, .94, .80, .74, .77, .93, .78, .79, .78, .92, .77, .77, .76, .92, .76, .71, .76, .92],\n",
    "                   'std': [0.01, 0.04, 0.03, 0.01, 0.02, 0.01, 0.03, 0.00, 0.01, 0.03, 0.02, 0.01, 0.03, 0.12, 0.02, 0.00, 0.03, 0.02, 0.03, 0.01, 0.02, 0.05, 0.04, 0.01, 0.02 ,0.11, 0.02, 0.00]}, index=range(0, 28))\n",
    "plt.figure()\n",
    "titles = []\n",
    "for model in [\n",
    "    {'key': 'LR', 'method': lr},\n",
    "    {'key': 'MLP', 'method': mlp},\n",
    "    {'key': 'Decision Tree', 'method': decision_tree},\n",
    "    {'key': 'CGN_3_layer_64_channel_emb_32_dropout', 'method': cgn_loop, 'num_channel': 64, 'num_layer': 3, 'add_emb': 32, 'use_gate': False, 'dropout': True, 'cuda': True},\n",
    "    ]:\n",
    "    temp_results = results_df.loc[results_df['model'] == model['key']].reset_index(drop=True)\n",
    "    lines.append(plt.errorbar(temp_results.index, temp_results['auc'], xerr=0, yerr=temp_results['std'])[0])\n",
    "    titles.append(model['key'])\n",
    "    plt.xticks(list(temp_results.index), temp_results['num_genes'], rotation=70)\n",
    "width = 0.2\n",
    "plt.title(\"Inferring the value of RPL5 with varying numbers of genes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"# genes\")\n",
    "plt.legend(lines, titles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results of adding Nodes\n",
    "plt.figure()\n",
    "\n",
    "#full_results.loc[full_results['samples'] == 100]\n",
    "\n",
    "line1 = plt.errorbar(lr_results.index, lr_results['auc'], xerr=0, yerr=lr_results['std'])\n",
    "line2 = plt.errorbar(cgn_results.index, cgn_results['auc'], xerr=0, yerr=cgn_results['std'])\n",
    "\n",
    "width = 0.2\n",
    "plt.xticks(list(lr_results.iloc[::5, :].index), lr_results.iloc[::5, :]['num_genes'], rotation=70)\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.legend((line1[0], line2[0]), ('LR', \"CGN\"), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict a gene from a growing number of Nodes\n",
    "lr_results = pd.DataFrame([])\n",
    "mlp_results = pd.DataFrame([])\n",
    "cgn_results = pd.DataFrame([])\n",
    "gene = \"RPL5\"\n",
    "max_samples = 200\n",
    "reload(data)\n",
    "reload(models)\n",
    "tcgatissue = data.gene_datasets.TCGATissue(data_dir='./genomics/TCGA/', data_file='TCGA_tissue_ppi.hdf5')\n",
    "\n",
    "for num_samples in range(10, max_samples, 20):\n",
    "    lr_row = infer_gene(lr, tcgatissue, \"RPL5\", train_size=num_samples, test_size=200, trials=3, penalty=True)\n",
    "    lr_results = lr_results.append(lr_row).reset_index(drop=True)\n",
    "    lr_results.loc[lr_results.index[-1], 'num_samples'] = num_samples\n",
    "    cgn_row = infer_gene(cgn, tcgatissue, \"RPL5\", train_size=num_samples, test_size=200, trials=3, penalty=True)\n",
    "    cgn_results = cgn_results.append(cgn_row).reset_index(drop=True)\n",
    "    cgn_results.loc[lr_results.index[-1], 'num_samples'] = num_samples\n",
    "    print num_genes\n",
    "    print cgn_results\n",
    "    print lr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results of adding Nodes\n",
    "plt.figure()\n",
    "\n",
    "#full_results.loc[full_results['samples'] == 100]\n",
    "\n",
    "line1 = plt.errorbar(lr_results.index, lr_results['auc'], xerr=0, yerr=lr_results['std'])\n",
    "line2 = plt.errorbar(cgn_results.index, cgn_results['auc'], xerr=0, yerr=cgn_results['std'])\n",
    "\n",
    "width = 0.2\n",
    "plt.xticks(list(lr_results.iloc[::5, :].index), lr_results.iloc[::5, :]['num_samples'], rotation=70)\n",
    "plt.title(\"Gene Inference with varying numbers of samples\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of samples\")\n",
    "plt.legend((line1[0], line2[0]), ('LR', \"CGN\"), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
