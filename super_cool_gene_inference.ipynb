{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import data, data.gene_datasets, data.colombos\n",
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import data.graph\n",
    "from data.graph import Graph\n",
    "import analysis\n",
    "from analysis import monitoring\n",
    "import main\n",
    "import optimization as otim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting one-hot labels to integers\n"
     ]
    }
   ],
   "source": [
    "dataset = data.gene_datasets.TCGAGeneInference()\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "opt = Object()\n",
    "opt.seed = 1993\n",
    "opt.nb_class = None\n",
    "opt.nb_examples = None\n",
    "opt.nb_nodes = None\n",
    "opt.graph = \"pathway\"\n",
    "opt.dataset = dataset\n",
    "opt.add_self = True\n",
    "opt.add_connectivity = False\n",
    "opt.batch_size = 128\n",
    "\n",
    "graph = Graph()\n",
    "path = \"/data/lisa/data/genomics/graph/pancan-tissue-graph.hdf5\"\n",
    "graph.load_graph(path)\n",
    "graph.intersection_with(dataset)\n",
    "g = nx.from_numpy_matrix(graph.adj)\n",
    "mapping = dict(zip(range(0, len(dataset.df.columns)), dataset.df.columns))\n",
    "g = nx.relabel_nodes(g, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10459, 5000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './experiments/experiments/tcga-tissue-gene-inference/cgn/testing123/329006fa5a858bcbac9ffe480bfc1485/1993/checkpoint.pth.tar'\n",
      "Loading the model with these parameters: <__main__.Object object at 0x7f2fe6a82550>\n",
      "=> loaded checkpoint './experiments/experiments/tcga-tissue-gene-inference/cgn/testing123/329006fa5a858bcbac9ffe480bfc1485/1993/checkpoint.pth.tar' (epoch 10)\n",
      "Doing drop-out\n",
      "Our model:\n",
      "CGN (\n",
      "  (emb): EmbeddingLayer (\n",
      "  )\n",
      "  (my_convs): ModuleList (\n",
      "    (0): CGNLayer (\n",
      "      (linear): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
      "      (eye_linear): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (my_dropouts): ModuleList (\n",
      "    (0): Dropout (p = 0.1)\n",
      "  )\n",
      "  (last_inference_layer): Conv1d(32, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "load_folder = \"./experiments/experiments/tcga-tissue-gene-inference/cgn/testing123/329006fa5a858bcbac9ffe480bfc1485/1993\"\n",
    "opt.load_folder = load_folder\n",
    "opt.load_checkpoint = True\n",
    "opt.epoch = 100\n",
    "opt.training_mode = 'gene-inference'\n",
    "my_model, optimizer, epoch, saved_opt = analysis.monitoring.load_checkpoint(load_folder, \n",
    "                                                                           opt, dataset, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "train_set, valid_set, test_set = main.split_dataset(dataset, batch_size=1, seed = saved_opt.seed,\n",
    "                                                   nb_samples=saved_opt.nb_examples, train_ratio=saved_opt.train_ratio, \n",
    "                                                    nb_per_class=saved_opt.nb_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(otim)\n",
    "\n",
    "def evaluate(my_model, eval_set, opt):\n",
    "    \n",
    "    dataset = eval_set.dataset\n",
    "    criterions = otim.get_criterion(dataset, opt.training_mode)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for no_b, mini in enumerate(eval_set):\n",
    "            inputs, targets = mini['sample'], mini['labels']\n",
    "\n",
    "            inputs = Variable(inputs, requires_grad=False).float()\n",
    "            #targets = Variable(targets, requires_grad=False).long()\n",
    "\n",
    "            if opt.cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            my_model.eval()\n",
    "\n",
    "            y_pred = my_model(inputs)\n",
    "\n",
    "            # Compute and print loss\n",
    "            crit_loss = otim.compute_loss(criterions, y_pred, targets, opt.training_mode, opt.semi_mse_lambda)\n",
    "            results.append(crit_loss.data.numpy()[0])\n",
    "            \n",
    "    return results\n",
    "        \n",
    "\n",
    "results = evaluate(my_model, valid_set, saved_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988,\n",
       " 0.15951988]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
