{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on those articles/blog:\n",
    "# pytoch implementation of cgn: https://arxiv.org/pdf/1609.02907.pdf\n",
    "# http://tkipf.github.io/graph-convolutional-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a lambda graph with adjancy matrix and stuff.\n",
    "nb_nodes = 10\n",
    "nb_edges = 10\n",
    "\n",
    "# nodes\n",
    "nodes = np.arange(nb_nodes)\n",
    "\n",
    "# roughly nb_edges edges\n",
    "edges = np.array([(i, ((((i + np.random.randint(nb_nodes - 1))  % nb_nodes) + 1 ) % nb_nodes ))\n",
    "                  for i in [np.random.randint(nb_nodes) for i in range(nb_edges)]])\n",
    "\n",
    "# Adding self loop.\n",
    "edges = np.concatenate((edges, np.array([(i, i) for i in nodes])))\n",
    "\n",
    "\n",
    "# adjacent matrix\n",
    "A = np.zeros((nb_nodes, nb_nodes))\n",
    "A[edges[:, 0], edges[:, 1]] = 1.\n",
    "A[edges[:, 1from torch.autograd import Variable], edges[:, 0]] = 1.\n",
    "\n",
    "# Degree matrix\n",
    "D = A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjency matrix: \n",
      "[[ 1.  0.  1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  0.  1.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  1.  1.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  0.  1.]\n",
      " [ 0.  0.  1.  1.  0.  0.  1.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  1.  0.  0.  1.]] \n",
      " nb edges: 30.0 \n",
      " Degree: [ 3.  2.  4.  3.  4.  2.  3.  4.  2.  3.] \n"
     ]
    }
   ],
   "source": [
    "print \"Adjency matrix: \\n{} \\n nb edges: {} \\n Degree: {} \".format(A, A.sum(), D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33  0.    0.29  0.    0.    0.    0.    0.    0.41  0.  ]\n",
      " [ 0.    0.5   0.    0.    0.    0.5   0.    0.    0.    0.  ]\n",
      " [ 0.29  0.    0.25  0.    0.25  0.    0.    0.25  0.    0.  ]\n",
      " [ 0.    0.    0.    0.33  0.29  0.    0.    0.29  0.    0.  ]\n",
      " [ 0.    0.    0.25  0.29  0.25  0.    0.    0.    0.    0.29]\n",
      " [ 0.    0.5   0.    0.    0.    0.5   0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.33  0.29  0.    0.33]\n",
      " [ 0.    0.    0.25  0.29  0.    0.    0.29  0.25  0.    0.  ]\n",
      " [ 0.41  0.    0.    0.    0.    0.    0.    0.    0.5   0.  ]\n",
      " [ 0.    0.    0.    0.    0.29  0.    0.33  0.    0.    0.33]]\n"
     ]
    }
   ],
   "source": [
    "# Get the Normalized matrix :D^(-1/2)AD^(-1/2)\n",
    "np.set_printoptions(precision=2)\n",
    "D_inv = np.diag(1./np.sqrt(D))\n",
    "norm_transform = D_inv.dot(A).dot(D_inv)\n",
    "print norm_transform\n",
    "# So it's not only an average, it's weighted by something (need to investigate why).\n",
    "# From what I can tell it's kind of a random walk throught the graph. \n",
    "# (i.e. the weights on edges connecting \"popular\" nodes are smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a module for the CGN:\n",
    "class CGN(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_nodes, input_dim, channels, D_norm, out_dim=None):\n",
    "        super(CGN, self).__init__()\n",
    "\n",
    "        self.my_layers = []\n",
    "        self.out_dim = out_dim\n",
    "        self.D_norm = Variable(torch.from_numpy(D_norm).float(), requires_grad=False) # The normalizing matrix.\n",
    "        \n",
    "        dims = [input_dim] + channels\n",
    "        \n",
    "        layers = []\n",
    "        for c_in, c_out in zip(dims[:-1], dims[1:]):\n",
    "            layer = nn.Linear(c_in, c_out)#Variable(torch.randn(c_in, c_out), requires_grad=True)\n",
    "            layers.append(layer)\n",
    "        self.my_layers = nn.ModuleList(layers)\n",
    "        \n",
    "        # If we have only one target per graph, we have a linear layer.\n",
    "        if out_dim is not None:\n",
    "            self.last_layer = nn.Linear(nb_nodes * channels[-1], out_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        def batch_mul(x, D):\n",
    "    \n",
    "            all_output_c = []\n",
    "            for i in range(x.size()[-1]):\n",
    "                all_output_c.append(x[:, :, i].mm(D))\n",
    "\n",
    "            return torch.stack(all_output_c).permute(1, 2, 0).contiguous()\n",
    "\n",
    "        nb_examples, nb_nodes = x.size()[0], x.size()[1] \n",
    "        \n",
    "        # Do graph convolution for all \n",
    "        for layer in self.my_layers:\n",
    "            \n",
    "            # Do the normalization (have to do tricky stack for the number of output. It suck a bit.)\n",
    "            x = batch_mul(x, self.D_norm)\n",
    "            #x = x.mm(torch.stack([self.D_norm] * x.size()[-1]))\n",
    "            \n",
    "            # Reshape to do elementwise, for all the node.\n",
    "            x = x.view(nb_examples*nb_nodes, -1)\n",
    "            x = F.relu(layer(x))\n",
    "            x = x.view(nb_examples, nb_nodes, -1)\n",
    "            \n",
    "            #x = F.relu(batch_mul(x,  self.D_norm).view(nb_nodes*nb_examples, -1).mm(w)).view(nb_examples, nb_nodes, -1)\n",
    "            \n",
    "        if self.out_dim is not None:\n",
    "            x = self.last_layer(x.view(nb_examples, -1))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some random data:\n",
    "nb_examples = 200 # examples\n",
    "nb_out = 1 # the umber of output (for classification)\n",
    "\n",
    "\n",
    "# Generate random stuff.\n",
    "inputs = Variable(torch.randn((nb_examples, nb_nodes, 1)), requires_grad=False)\n",
    "targets = Variable(torch.randn((nb_examples, nb_out)), requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model:\n",
      "CGN (\n",
      "  (my_layers): ModuleList (\n",
      "    (0): Linear (1 -> 16)\n",
      "    (1): Linear (16 -> 16)\n",
      "    (2): Linear (16 -> 16)\n",
      "  )\n",
      "  (last_layer): Linear (160 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create our model.\n",
    "cgn = CGN(nb_nodes, 1, [16, 16, 16], norm_transform, nb_out)\n",
    "\n",
    "print \"Our model:\"\n",
    "print cgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 177.50282287597656)\n",
      "(500, 156.74127197265625)\n",
      "(1000, 135.6536407470703)\n",
      "(1500, 108.57597351074219)\n",
      "(2000, 84.69371032714844)\n",
      "(2500, 59.66895294189453)\n",
      "(3000, 43.29962921142578)\n",
      "(3500, 32.25688934326172)\n",
      "(4000, 20.712610244750977)\n",
      "(4500, 15.861947059631348)\n"
     ]
    }
   ],
   "source": [
    "# Train the cgn\n",
    "learning_rate = 1e-4\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(cgn.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "epoch = 5000\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = cgn(inputs)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, targets)\n",
    "    \n",
    "    if t % (epoch/10) == 0:\n",
    "        print(t, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.7260\n",
      " 0.1509\n",
      "-0.2850\n",
      " 0.8225\n",
      "-1.2579\n",
      "-0.1841\n",
      " 0.5832\n",
      "-0.8808\n",
      " 1.7042\n",
      "-0.6613\n",
      "[torch.FloatTensor of size 10x1]\n",
      " Variable containing:\n",
      "-1.9094\n",
      " 0.0164\n",
      "-0.4924\n",
      " 0.9970\n",
      "-1.0331\n",
      "-0.3450\n",
      " 0.4138\n",
      "-0.8928\n",
      " 1.8359\n",
      "-0.3402\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the results, and compare them\n",
    "outputs = cgn(inputs)\n",
    "print outputs[:10], targets[:10]\n",
    "# Good enough for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
