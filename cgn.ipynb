{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on those articles/blog:\n",
    "# pytoch implementation of cgn: https://arxiv.org/pdf/1609.02907.pdf\n",
    "# http://tkipf.github.io/graph-convolutional-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a lambda graph with adjancy matrix and stuff.\n",
    "nb_nodes = 10\n",
    "nb_edges = 20\n",
    "\n",
    "# nodes\n",
    "nodes = np.arange(nb_nodes)\n",
    "\n",
    "# roughly nb_edges edges\n",
    "edges = np.array([(i, ((((i + np.random.randint(nb_nodes - 1))  % nb_nodes) + 1 ) % nb_nodes ))\n",
    "                  for i in [np.random.randint(nb_nodes) for i in range(nb_edges)]])\n",
    "\n",
    "# Adding self loop.\n",
    "edges = np.concatenate((edges, np.array([(i, i) for i in nodes])))\n",
    "\n",
    "\n",
    "# adjacent matrix\n",
    "A = np.zeros((nb_nodes, nb_nodes))\n",
    "A[edges[:, 0], edges[:, 1]] = 1.\n",
    "A[edges[:, 1], edges[:, 0]] = 1.\n",
    "\n",
    "# Degree matrix\n",
    "D = A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjency matrix: \n",
      "[[ 1.  0.  1.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  1.  1.  1.  0.  0.  0.]\n",
      " [ 1.  1.  1.  0.  1.  1.  0.  1.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  1.  0.  0.  1.  1.  0.]\n",
      " [ 1.  1.  1.  0.  0.  1.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  1.  0.  1.  1.]\n",
      " [ 0.  0.  1.  0.  1.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  1.  1.  1.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  1.  1.]] \n",
      " nb edges: 42.0 \n",
      " Degree: [ 3.  5.  7.  1.  5.  5.  4.  3.  6.  3.] \n"
     ]
    }
   ],
   "source": [
    "print \"Adjency matrix: \\n{} \\n nb edges: {} \\n Degree: {} \".format(A, A.sum(), D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33  0.    0.22  0.    0.    0.26  0.    0.    0.    0.  ]\n",
      " [ 0.    0.2   0.17  0.    0.2   0.2   0.22  0.    0.    0.  ]\n",
      " [ 0.22  0.17  0.14  0.    0.17  0.17  0.    0.22  0.15  0.  ]\n",
      " [ 0.    0.    0.    1.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.2   0.17  0.    0.2   0.    0.    0.26  0.18  0.  ]\n",
      " [ 0.26  0.2   0.17  0.    0.    0.2   0.    0.    0.18  0.  ]\n",
      " [ 0.    0.22  0.    0.    0.    0.    0.25  0.    0.2   0.29]\n",
      " [ 0.    0.    0.22  0.    0.26  0.    0.    0.33  0.    0.  ]\n",
      " [ 0.    0.    0.15  0.    0.18  0.18  0.2   0.    0.17  0.24]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.29  0.    0.24  0.33]]\n"
     ]
    }
   ],
   "source": [
    "# Get the Normalized matrix :D^(-1/2)AD^(-1/2)\n",
    "np.set_printoptions(precision=2)\n",
    "D_inv = np.diag(1./np.sqrt(D))\n",
    "norm_transform = D_inv.dot(A).dot(D_inv)\n",
    "print norm_transform\n",
    "# So it's not only an average, it's weighted by something (need to investigate why).\n",
    "# From what I can tell it's kind of a random walk throught the graph. \n",
    "# (i.e. the weights on edges connecting \"popular\" nodes are smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a module for the CGN:\n",
    "class CGN(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_nodes, input_dim, channels, D_norm, out_dim=None,\n",
    "                ):\n",
    "        super(CGN, self).__init__()\n",
    "\n",
    "        self.my_layers = []\n",
    "        self.out_dim = out_dim\n",
    "        self.D_norm = Variable(torch.from_numpy(D_norm).float(), requires_grad=False) # The normalizing matrix.\n",
    "        dims = [input_dim] + channels\n",
    "        \n",
    "        layers = []\n",
    "        for c_in, c_out in zip(dims[:-1], dims[1:]):\n",
    "            layer = nn.Linear(c_in, c_out)#Variable(torch.randn(c_in, c_out), requires_grad=True)\n",
    "            layers.append(layer)\n",
    "        self.my_layers = nn.ModuleList(layers)\n",
    "        \n",
    "        # If we have only one target per graph, we have a linear layer.\n",
    "        if out_dim is not None:\n",
    "            self.last_layer = nn.Linear(nb_nodes * channels[-1], out_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        def batch_mul(x, D):\n",
    "    \n",
    "            all_output_c = []\n",
    "            for i in range(x.size()[-1]):\n",
    "                all_output_c.append(x[:, :, i].mm(D))\n",
    "\n",
    "            return torch.stack(all_output_c).permute(1, 2, 0).contiguous()\n",
    "\n",
    "        nb_examples, nb_nodes = x.size()[0], x.size()[1] \n",
    "        \n",
    "        # Do graph convolution for all \n",
    "        for layer in self.my_layers:\n",
    "            \n",
    "            # Do the normalization (have to do tricky stack for the number of output. It suck a bit.)\n",
    "            # We should change here to do a kind of convolution or something.\n",
    "            x = batch_mul(x, self.D_norm)\n",
    "            \n",
    "            # Reshape to do elementwise, for all the node.\n",
    "            x = x.view(nb_examples*nb_nodes, -1)\n",
    "            x = F.tanh(layer(x)) # or relu, sigmoid...\n",
    "            x = x.view(nb_examples, nb_nodes, -1)\n",
    "            \n",
    "        if self.out_dim is not None:\n",
    "            x = self.last_layer(x.view(nb_examples, -1))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate some random data:\n",
    "nb_examples = 1000 # examples\n",
    "nb_out = 1 # the umber of output (for classification)\n",
    "\n",
    "\n",
    "# Generate random stuff.\n",
    "inputs = Variable(torch.randn((nb_examples, nb_nodes, 1)), requires_grad=False)\n",
    "#targets = Variable(torch.randn((nb_examples, nb_out)), requires_grad=False)\n",
    "targets = Variable(torch.sum(inputs.data, dim=1), requires_grad=False).squeeze() # try to predict the sum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model:\n",
      "CGN (\n",
      "  (my_layers): ModuleList (\n",
      "    (0): Linear (1 -> 16)\n",
      "    (1): Linear (16 -> 16)\n",
      "    (2): Linear (16 -> 16)\n",
      "  )\n",
      "  (last_layer): Linear (160 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create our model.\n",
    "cgn = CGN(nb_nodes, 1, [16] * 3, norm_transform, nb_out)\n",
    "\n",
    "print \"Our model:\"\n",
    "print cgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 9.584810256958008)\n",
      "(50, 9.020203590393066)\n",
      "(100, 7.8015336990356445)\n",
      "(150, 5.213669776916504)\n",
      "(200, 1.8049923181533813)\n",
      "(250, 0.581387460231781)\n",
      "(300, 0.47112777829170227)\n",
      "(350, 0.436573326587677)\n",
      "(400, 0.41667523980140686)\n",
      "(450, 0.40431204438209534)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Train the cgn\n",
    "learning_rate = 1e-4\n",
    "criterion = torch.nn.MSELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(cgn.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "epoch = 500\n",
    "for t in range(epoch):\n",
    "    \n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = cgn(inputs)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, targets)\n",
    "    \n",
    "    if t % (epoch/10) == 0:\n",
    "        print(t, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-6.8392\n",
      " 1.8050\n",
      " 2.1839\n",
      " 4.1821\n",
      " 1.5968\n",
      "-2.6059\n",
      " 1.9223\n",
      "-4.5044\n",
      " 0.5823\n",
      "-0.2158\n",
      "[torch.FloatTensor of size 10x1]\n",
      " Variable containing:\n",
      "-7.6236\n",
      " 0.6695\n",
      " 2.8357\n",
      " 4.1461\n",
      " 1.9421\n",
      "-2.3159\n",
      " 1.1496\n",
      "-4.5373\n",
      " 0.0308\n",
      " 0.3014\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the results, and compare them\n",
    "outputs = cgn(inputs)\n",
    "print outputs[:10], targets[:10]\n",
    "# Good enough for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
