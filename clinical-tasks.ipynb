{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import sklearn\n",
    "import torch\n",
    "import datetime\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from models.model_wrapper import MLP\n",
    "from data import datasets\n",
    "from data.gene_graphs import GeneManiaGraph, RegNetGraph\n",
    "from data.utils import record_result\n",
    "from data.clinical.datasets import TCGADataset, Task\n",
    "from data.clinical import taskloader\n",
    "from data.clinical import split_dataset\n",
    "\n",
    "from models.model_wrapper import MLP, GCN, SLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torrent name: TCGA_tissue_ppi.hdf5, Size: 1748.32MB\n",
      "Checking for pieces on disk: |██████████████████████████████████████████████████| 100.0% \n",
      "Found 1668 finished pieces out of 1668 total pieces.\n",
      "Found dataset at /Users/martinweiss/.academictorrents-datastore/TCGA_tissue_ppi.hdf5\n"
     ]
    }
   ],
   "source": [
    "tcga = TCGADataset()\n",
    "task_id = \"_EVENT-BRCA\"\n",
    "tasks = [Task(tcga, task_id, limit=1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torrent name: genemania.pkl, Size: 9.61MB\n"
     ]
    }
   ],
   "source": [
    "graphs = {\"genemania\": GeneManiaGraph()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Checkpointed Results\n"
     ]
    }
   ],
   "source": [
    "# Setup the results dictionary\n",
    "filename = \"experiments/results/graph-gen-clinical.pkl\"\n",
    "try:\n",
    "    results = pickle.load(open(filename, \"rb\"), encoding='latin1')\n",
    "    print(\"Loaded Checkpointed Results\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    results = pd.DataFrame(columns=['task', 'auc', 'gene', 'model', 'graph', 'seed', 'train_size', 'optimize_graph_results'])\n",
    "    print(\"Created a New Results Dictionary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 50\n",
    "test_size = 500\n",
    "trials = 3\n",
    "cuda = False\n",
    "models = [           \n",
    "              GCN(name=\"GCN_lay20_chan32_emb32_dropout_pool\", cuda=cuda, num_layer=4, channels=32, embedding=32, prepool_extralayers=5, pooling=\"hierarchy\"),\n",
    "              GCN(name=\"GCN_lay3_chan64_emb32_dropout\", cuda=cuda, num_layer=3, channels=64, embedding=32),\n",
    "              MLP(name=\"MLP_lay2_chan512_dropout\", cuda=cuda, dropout=True, num_layer=2, channels=512),\n",
    "              MLP(name=\"MLP_lay2_chan512\", cuda=cuda, dropout=False, num_layer=2, channels=512),\n",
    "              SLR(name=\"SLR_lambda1_l11\", cuda=cuda)\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "todo: 15\n",
      "done: 2\n"
     ]
    }
   ],
   "source": [
    "# Create the set of all experiment ids and see which are left to do\n",
    "columns = [\"task\", \"graph\", \"model\", \"seed\", \"train_size\"]\n",
    "all_exp_ids = [x for x in itertools.product(tasks, graphs.keys(), models, range(trials), [train_size])]\n",
    "all_exp_ids = pd.DataFrame(all_exp_ids, columns=columns)\n",
    "all_exp_ids.index = [\"-\".join(map(str, tup[1:])) for tup in all_exp_ids.itertuples(name=None)]\n",
    "results_exp_ids = results[columns].copy()\n",
    "results_exp_ids.index = [\"-\".join(map(str, tup[1:])) for tup in results_exp_ids.itertuples(name=None)]\n",
    "intersection_ids = all_exp_ids.index.intersection(results_exp_ids.index)\n",
    "todo = all_exp_ids.drop(intersection_ids).to_dict(orient=\"records\")\n",
    "\n",
    "print(\"todo: \" + str(len(todo)))\n",
    "print(\"done: \" + str(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_graph(model, gene_graph, gene, X_train, X_test, y_train, y_test):\n",
    "    neighbors = list(gene_graph.first_degree(gene)[0])\n",
    "    neighbors = [n for n in neighbors if n in X_train.columns.values]\n",
    "    res = {}\n",
    "    for neighbor in neighbors:\n",
    "        candidate_nodes = neighbors if gene == neighbor else set(neighbors) - set(neighbor)\n",
    "        x_train = X_train.loc[:, candidate_nodes].copy()\n",
    "        x_test = X_test.loc[:, candidate_nodes].copy()\n",
    "        x_train[gene] = 1\n",
    "        x_test[gene] = 1\n",
    "\n",
    "        try:\n",
    "            model.fit(x_train, y_train)\n",
    "            x_test = Variable(torch.FloatTensor(np.expand_dims(x_test.values, axis=2)), requires_grad=False).float()\n",
    "            if cuda:\n",
    "                x_test = x_test.cuda()\n",
    "            y_hat = model.predict(x_test)[:, 1].data.cpu().numpy()\n",
    "            auc = sklearn.metrics.roc_auc_score(y_test, np.asarray(y_hat).flatten())\n",
    "            res[neighbor] = auc\n",
    "            model.best_model = None # cleanup\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinweiss/code/academic/conv-graph/venv/lib/python3.5/site-packages/sklearn/cluster/hierarchical.py:244: UserWarning: the number of connected components of the connectivity matrix is 9 > 1. Completing it to avoid stopping the tree early.\n",
      "  affinity='euclidean')\n",
      "/Users/martinweiss/code/academic/conv-graph/venv/lib/python3.5/site-packages/sklearn/cluster/hierarchical.py:831: UserWarning: Persisting input arguments took 2.99s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **kwargs)\n",
      "/Users/martinweiss/code/academic/conv-graph/venv/lib/python3.5/site-packages/sklearn/cluster/hierarchical.py:244: UserWarning: the number of connected components of the connectivity matrix is 5 > 1. Completing it to avoid stopping the tree early.\n",
      "  affinity='euclidean')\n"
     ]
    }
   ],
   "source": [
    "optimize_graph_results = []\n",
    "\n",
    "for row in todo:\n",
    "    if len(results) % 10 == 0:\n",
    "        print(len(results))\n",
    "    task = row[\"task\"]\n",
    "    graph_name = row[\"graph\"]\n",
    "    seed = row[\"seed\"]\n",
    "    model = row[\"model\"]\n",
    "\n",
    "    experiment = {\n",
    "        \"task\": task.id,\n",
    "        \"model\": model.name,\n",
    "        \"graph\": graph_name,\n",
    "        \"seed\": seed,\n",
    "        \"train_size\": train_size,\n",
    "        \"optimize_graph_results\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.\\\n",
    "            train_test_split(task.data, task.labels, stratify=task.labels, \n",
    "                             train_size=train_size, test_size=test_size)\n",
    "    except ValueError:\n",
    "        import pdb; pdb.set_trace()\n",
    "        results = record_result(results, experiment, filename)\n",
    "        continue\n",
    "\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    try:\n",
    "        gene_graph = graphs[graph_name]\n",
    "        adj = np.asarray(nx.to_numpy_matrix(gene_graph.nx_graph))\n",
    "        model.fit(X_train, y_train.astype(\"uint8\"), adj=adj)\n",
    "\n",
    "        x_test = Variable(torch.FloatTensor(np.expand_dims(X_test.values, axis=2)), requires_grad=False).float()\n",
    "        if cuda:\n",
    "            x_test = x_test.cuda()\n",
    "        y_hat = model.predict(x_test)[:, 1].data.cpu().numpy()\n",
    "        auc = sklearn.metrics.roc_auc_score(y_test.astype(\"uint8\"), np.asarray(y_hat).flatten())\n",
    "        model.best_model = None # cleanup\n",
    "        experiment[\"auc\"] = auc\n",
    "    except Exception as e:\n",
    "        import pdb; pdb.set_trace()\n",
    "        print(e)\n",
    "        \n",
    "    results = record_result(results, experiment, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
